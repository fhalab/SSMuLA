{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/disk2/fli/SSMuLA\n"
     ]
    }
   ],
   "source": [
    "%cd ~/SSMuLA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A script to run the COVES algorithm on a given dataset.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "from glob import glob\n",
    "import tqdm\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import torch, functools\n",
    "from torch import nn, scatter_add\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_cluster\n",
    "from torch.utils.data import IterableDataset\n",
    "import torch, random, scipy, math\n",
    "\n",
    "from atom3d.datasets import LMDBDataset\n",
    "import atom3d.datasets.datasets as da\n",
    "import atom3d.splits.splits as spl\n",
    "import atom3d.util.file as fi\n",
    "import atom3d.util.formats as fo\n",
    "from atom3d.util import metrics\n",
    "\n",
    "from SSMuLA.util import checkNgen_folder, get_file_name, read_fasta\n",
    "\n",
    "# to go from 3 letter amino acid code to one letter amino acid code\n",
    "AA3_TO_AA1 = {\n",
    "    \"CYS\": \"C\",\n",
    "    \"ASP\": \"D\",\n",
    "    \"SER\": \"S\",\n",
    "    \"GLN\": \"Q\",\n",
    "    \"LYS\": \"K\",\n",
    "    \"ILE\": \"I\",\n",
    "    \"PRO\": \"P\",\n",
    "    \"THR\": \"T\",\n",
    "    \"PHE\": \"F\",\n",
    "    \"ASN\": \"N\",\n",
    "    \"GLY\": \"G\",\n",
    "    \"HIS\": \"H\",\n",
    "    \"LEU\": \"L\",\n",
    "    \"ARG\": \"R\",\n",
    "    \"TRP\": \"W\",\n",
    "    \"ALA\": \"A\",\n",
    "    \"VAL\": \"V\",\n",
    "    \"GLU\": \"E\",\n",
    "    \"TYR\": \"Y\",\n",
    "    \"MET\": \"M\",\n",
    "}\n",
    "\n",
    "AA1_TO_AA3 = dict(zip(AA3_TO_AA1.values(), AA3_TO_AA1.keys()))\n",
    "\n",
    "_amino_acids = lambda x: {\n",
    "    \"ALA\": 0,\n",
    "    \"ARG\": 1,\n",
    "    \"ASN\": 2,\n",
    "    \"ASP\": 3,\n",
    "    \"CYS\": 4,\n",
    "    \"GLU\": 5,\n",
    "    \"GLN\": 6,\n",
    "    \"GLY\": 7,\n",
    "    \"HIS\": 8,\n",
    "    \"ILE\": 9,\n",
    "    \"LEU\": 10,\n",
    "    \"LYS\": 11,\n",
    "    \"MET\": 12,\n",
    "    \"PHE\": 13,\n",
    "    \"PRO\": 14,\n",
    "    \"SER\": 15,\n",
    "    \"THR\": 16,\n",
    "    \"TRP\": 17,\n",
    "    \"TYR\": 18,\n",
    "    \"VAL\": 19,\n",
    "}.get(x, 20)\n",
    "\n",
    "\n",
    "aa3_to_num = {\n",
    "    \"ALA\": 0,\n",
    "    \"ARG\": 1,\n",
    "    \"ASN\": 2,\n",
    "    \"ASP\": 3,\n",
    "    \"CYS\": 4,\n",
    "    \"GLU\": 5,\n",
    "    \"GLN\": 6,\n",
    "    \"GLY\": 7,\n",
    "    \"HIS\": 8,\n",
    "    \"ILE\": 9,\n",
    "    \"LEU\": 10,\n",
    "    \"LYS\": 11,\n",
    "    \"MET\": 12,\n",
    "    \"PHE\": 13,\n",
    "    \"PRO\": 14,\n",
    "    \"SER\": 15,\n",
    "    \"THR\": 16,\n",
    "    \"TRP\": 17,\n",
    "    \"TYR\": 18,\n",
    "    \"VAL\": 19,\n",
    "}\n",
    "\n",
    "num_to_aa3 = dict(zip(aa3_to_num.values(), aa3_to_num.keys()))\n",
    "\n",
    "label_res_dict = {\n",
    "    0: \"HIS\",\n",
    "    1: \"LYS\",\n",
    "    2: \"ARG\",\n",
    "    3: \"ASP\",\n",
    "    4: \"GLU\",\n",
    "    5: \"SER\",\n",
    "    6: \"THR\",\n",
    "    7: \"ASN\",\n",
    "    8: \"GLN\",\n",
    "    9: \"ALA\",\n",
    "    10: \"VAL\",\n",
    "    11: \"LEU\",\n",
    "    12: \"ILE\",\n",
    "    13: \"MET\",\n",
    "    14: \"PHE\",\n",
    "    15: \"TYR\",\n",
    "    16: \"TRP\",\n",
    "    17: \"PRO\",\n",
    "    18: \"GLY\",\n",
    "    19: \"CYS\",\n",
    "}\n",
    "res_label_dict = {\n",
    "    \"HIS\": 0,\n",
    "    \"LYS\": 1,\n",
    "    \"ARG\": 2,\n",
    "    \"ASP\": 3,\n",
    "    \"GLU\": 4,\n",
    "    \"SER\": 5,\n",
    "    \"THR\": 6,\n",
    "    \"ASN\": 7,\n",
    "    \"GLN\": 8,\n",
    "    \"ALA\": 9,\n",
    "    \"VAL\": 10,\n",
    "    \"LEU\": 11,\n",
    "    \"ILE\": 12,\n",
    "    \"MET\": 13,\n",
    "    \"PHE\": 14,\n",
    "    \"TYR\": 15,\n",
    "    \"TRP\": 16,\n",
    "    \"PRO\": 17,\n",
    "    \"GLY\": 18,\n",
    "    \"CYS\": 19,\n",
    "}\n",
    "bb_atoms = [\"N\", \"CA\", \"C\", \"O\"]\n",
    "allowed_atoms = [\"C\", \"O\", \"N\", \"S\", \"P\", \"SE\"]\n",
    "\n",
    "# computed statistics from training set\n",
    "res_wt_dict = {\n",
    "    \"HIS\": 0.581391659111514,\n",
    "    \"LYS\": 0.266061611865989,\n",
    "    \"ARG\": 0.2796785729861747,\n",
    "    \"ASP\": 0.26563454667840314,\n",
    "    \"GLU\": 0.22814679094919596,\n",
    "    \"SER\": 0.2612916369563003,\n",
    "    \"THR\": 0.27832512315270935,\n",
    "    \"ASN\": 0.3477441570413752,\n",
    "    \"GLN\": 0.37781509139381086,\n",
    "    \"ALA\": 0.20421144813311043,\n",
    "    \"VAL\": 0.22354397064847012,\n",
    "    \"LEU\": 0.18395198072344454,\n",
    "    \"ILE\": 0.2631600545792168,\n",
    "    \"MET\": 0.6918305148744505,\n",
    "    \"PHE\": 0.3592224851905275,\n",
    "    \"TYR\": 0.4048964515721682,\n",
    "    \"TRP\": 0.9882874205355423,\n",
    "    \"PRO\": 0.32994186046511625,\n",
    "    \"GLY\": 0.2238561093317741,\n",
    "    \"CYS\": 1.0,\n",
    "}\n",
    "\n",
    "gly_CB_mu = np.array([-0.5311191, -0.75842446, 1.2198311], dtype=np.float32)\n",
    "gly_CB_sigma = np.array(\n",
    "    [\n",
    "        [1.63731114e-03, 2.40018381e-04, 6.38361679e-04],\n",
    "        [2.40018381e-04, 6.87853419e-05, 1.43898267e-04],\n",
    "        [6.38361679e-04, 1.43898267e-04, 3.25022011e-04],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "\n",
    "_NUM_ATOM_TYPES = 9\n",
    "_element_mapping = lambda x: {\n",
    "    \"H\": 0,\n",
    "    \"C\": 1,\n",
    "    \"N\": 2,\n",
    "    \"O\": 3,\n",
    "    \"F\": 4,\n",
    "    \"S\": 5,\n",
    "    \"Cl\": 6,\n",
    "    \"CL\": 6,\n",
    "    \"P\": 7,\n",
    "}.get(x, 8)\n",
    "\n",
    "_DEFAULT_V_DIM = (100, 16)\n",
    "_DEFAULT_E_DIM = (32, 1)\n",
    "\n",
    "\n",
    "def tuple_sum(*args):\n",
    "    \"\"\"\n",
    "    Sums any number of tuples (s, V) elementwise.\n",
    "    \"\"\"\n",
    "    return tuple(map(sum, zip(*args)))\n",
    "\n",
    "\n",
    "def tuple_cat(*args, dim=-1):\n",
    "    \"\"\"\n",
    "    Concatenates any number of tuples (s, V) elementwise.\n",
    "\n",
    "    :param dim: dimension along which to concatenate when viewed\n",
    "                as the `dim` index for the scalar-channel tensors.\n",
    "                This means that `dim=-1` will be applied as\n",
    "                `dim=-2` for the vector-channel tensors.\n",
    "    \"\"\"\n",
    "    dim %= len(args[0][0].shape)\n",
    "    s_args, v_args = list(zip(*args))\n",
    "    return torch.cat(s_args, dim=dim), torch.cat(v_args, dim=dim)\n",
    "\n",
    "\n",
    "def tuple_index(x, idx):\n",
    "    \"\"\"\n",
    "    Indexes into a tuple (s, V) along the first dimension.\n",
    "\n",
    "    :param idx: any object which can be used to index into a `torch.Tensor`\n",
    "    \"\"\"\n",
    "    return x[0][idx], x[1][idx]\n",
    "\n",
    "\n",
    "def _split(x, nv):\n",
    "    \"\"\"\n",
    "    Splits a merged representation of (s, V) back into a tuple.\n",
    "    Should be used only with `_merge(s, V)` and only if the tuple\n",
    "    representation cannot be used.\n",
    "\n",
    "    :param x: the `torch.Tensor` returned from `_merge`\n",
    "    :param nv: the number of vector channels in the input to `_merge`\n",
    "    \"\"\"\n",
    "    v = torch.reshape(x[..., -3 * nv :], x.shape[:-1] + (nv, 3))\n",
    "    s = x[..., : -3 * nv]\n",
    "    return s, v\n",
    "\n",
    "\n",
    "def _merge(s, v):\n",
    "    \"\"\"\n",
    "    Merges a tuple (s, V) into a single `torch.Tensor`, where the\n",
    "    vector channels are flattened and appended to the scalar channels.\n",
    "    Should be used only if the tuple representation cannot be used.\n",
    "    Use `_split(x, nv)` to reverse.\n",
    "    \"\"\"\n",
    "    v = torch.reshape(v, v.shape[:-2] + (3 * v.shape[-2],))\n",
    "    return torch.cat([s, v], -1)\n",
    "\n",
    "\n",
    "def _norm_no_nan(x, axis=-1, keepdims=False, eps=1e-8, sqrt=True):\n",
    "    \"\"\"\n",
    "    L2 norm of tensor clamped above a minimum value `eps`.\n",
    "\n",
    "    :param sqrt: if `False`, returns the square of the L2 norm\n",
    "    \"\"\"\n",
    "    out = torch.clamp(torch.sum(torch.square(x), axis, keepdims), min=eps)\n",
    "    return torch.sqrt(out) if sqrt else out\n",
    "\n",
    "\n",
    "class _VDropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector channel dropout where the elements of each\n",
    "    vector channel are dropped together.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_rate):\n",
    "        super(_VDropout, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: `torch.Tensor` corresponding to vector channels\n",
    "        \"\"\"\n",
    "        device = self.dummy_param.device\n",
    "        if not self.training:\n",
    "            return x\n",
    "        mask = torch.bernoulli(\n",
    "            (1 - self.drop_rate) * torch.ones(x.shape[:-1], device=device)\n",
    "        ).unsqueeze(-1)\n",
    "        x = mask * x / (1 - self.drop_rate)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined LayerNorm for tuples (s, V).\n",
    "    Takes tuples (s, V) as input and as output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.s, self.v = dims\n",
    "        self.scalar_norm = nn.LayerNorm(self.s)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tuple (s, V) of `torch.Tensor`,\n",
    "                  or single `torch.Tensor`\n",
    "                  (will be assumed to be scalar channels)\n",
    "        \"\"\"\n",
    "        if not self.v:\n",
    "            return self.scalar_norm(x)\n",
    "        s, v = x\n",
    "        vn = _norm_no_nan(v, axis=-1, keepdims=True, sqrt=False)\n",
    "        vn = torch.sqrt(torch.mean(vn, dim=-2, keepdim=True))\n",
    "        return self.scalar_norm(s), v / vn\n",
    "\n",
    "\n",
    "class Dropout(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined dropout for tuples (s, V).\n",
    "    Takes tuples (s, V) as input and as output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.sdropout = nn.Dropout(drop_rate)\n",
    "        self.vdropout = _VDropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tuple (s, V) of `torch.Tensor`,\n",
    "                  or single `torch.Tensor`\n",
    "                  (will be assumed to be scalar channels)\n",
    "        \"\"\"\n",
    "        if type(x) is torch.Tensor:\n",
    "            return self.sdropout(x)\n",
    "        s, v = x\n",
    "        return self.sdropout(s), self.vdropout(v)\n",
    "\n",
    "\n",
    "class GVP(nn.Module):\n",
    "    \"\"\"\n",
    "    Geometric Vector Perceptron. See manuscript and README.md\n",
    "    for more details.\n",
    "\n",
    "    :param in_dims: tuple (n_scalar, n_vector)\n",
    "    :param out_dims: tuple (n_scalar, n_vector)\n",
    "    :param h_dim: intermediate number of vector channels, optional\n",
    "    :param activations: tuple of functions (scalar_act, vector_act)\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dims,\n",
    "        out_dims,\n",
    "        h_dim=None,\n",
    "        activations=(F.relu, torch.sigmoid),\n",
    "        vector_gate=False,\n",
    "    ):\n",
    "        super(GVP, self).__init__()\n",
    "        self.si, self.vi = in_dims\n",
    "        self.so, self.vo = out_dims\n",
    "        self.vector_gate = vector_gate\n",
    "        if self.vi:\n",
    "            self.h_dim = h_dim or max(self.vi, self.vo)\n",
    "            self.wh = nn.Linear(self.vi, self.h_dim, bias=False)\n",
    "            self.ws = nn.Linear(self.h_dim + self.si, self.so)\n",
    "            if self.vo:\n",
    "                self.wv = nn.Linear(self.h_dim, self.vo, bias=False)\n",
    "                if self.vector_gate:\n",
    "                    self.wsv = nn.Linear(self.so, self.vo)\n",
    "        else:\n",
    "            self.ws = nn.Linear(self.si, self.so)\n",
    "\n",
    "        self.scalar_act, self.vector_act = activations\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: tuple (s, V) of `torch.Tensor`,\n",
    "                  or (if vectors_in is 0), a single `torch.Tensor`\n",
    "        :return: tuple (s, V) of `torch.Tensor`,\n",
    "                 or (if vectors_out is 0), a single `torch.Tensor`\n",
    "        \"\"\"\n",
    "        if self.vi:\n",
    "            s, v = x\n",
    "            v = torch.transpose(v, -1, -2)\n",
    "            vh = self.wh(v)\n",
    "            vn = _norm_no_nan(vh, axis=-2)\n",
    "            s = self.ws(torch.cat([s, vn], -1))\n",
    "            if self.vo:\n",
    "                v = self.wv(vh)\n",
    "                v = torch.transpose(v, -1, -2)\n",
    "                if self.vector_gate:\n",
    "                    if self.vector_act:\n",
    "                        gate = self.wsv(self.vector_act(s))\n",
    "                    else:\n",
    "                        gate = self.wsv(s)\n",
    "                    v = v * torch.sigmoid(gate).unsqueeze(-1)\n",
    "                elif self.vector_act:\n",
    "                    v = v * self.vector_act(_norm_no_nan(v, axis=-1, keepdims=True))\n",
    "        else:\n",
    "            s = self.ws(x)\n",
    "            if self.vo:\n",
    "                v = torch.zeros(s.shape[0], self.vo, 3, device=self.dummy_param.device)\n",
    "        if self.scalar_act:\n",
    "            s = self.scalar_act(s)\n",
    "\n",
    "        return (s, v) if self.vo else s\n",
    "\n",
    "\n",
    "class GVPConv(MessagePassing):\n",
    "    \"\"\"\n",
    "    Graph convolution / message passing with Geometric Vector Perceptrons.\n",
    "    Takes in a graph with node and edge embeddings,\n",
    "    and returns new node embeddings.\n",
    "\n",
    "    This does NOT do residual updates and pointwise feedforward layers\n",
    "    ---see `GVPConvLayer`.\n",
    "\n",
    "    :param in_dims: input node embedding dimensions (n_scalar, n_vector)\n",
    "    :param out_dims: output node embedding dimensions (n_scalar, n_vector)\n",
    "    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)\n",
    "    :param n_layers: number of GVPs in the message function\n",
    "    :param module_list: preconstructed message function, overrides n_layers\n",
    "    :param aggr: should be \"add\" if some incoming edges are masked, as in\n",
    "                 a masked autoregressive decoder architecture, otherwise \"mean\"\n",
    "    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dims,\n",
    "        out_dims,\n",
    "        edge_dims,\n",
    "        n_layers=3,\n",
    "        module_list=None,\n",
    "        aggr=\"mean\",\n",
    "        activations=(F.relu, torch.sigmoid),\n",
    "        vector_gate=False,\n",
    "    ):\n",
    "        super(GVPConv, self).__init__(aggr=aggr)\n",
    "        self.si, self.vi = in_dims\n",
    "        self.so, self.vo = out_dims\n",
    "        self.se, self.ve = edge_dims\n",
    "\n",
    "        GVP_ = functools.partial(GVP, activations=activations, vector_gate=vector_gate)\n",
    "\n",
    "        module_list = module_list or []\n",
    "        if not module_list:\n",
    "            if n_layers == 1:\n",
    "                module_list.append(\n",
    "                    GVP_(\n",
    "                        (2 * self.si + self.se, 2 * self.vi + self.ve),\n",
    "                        (self.so, self.vo),\n",
    "                        activations=(None, None),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                module_list.append(\n",
    "                    GVP_((2 * self.si + self.se, 2 * self.vi + self.ve), out_dims)\n",
    "                )\n",
    "                for i in range(n_layers - 2):\n",
    "                    module_list.append(GVP_(out_dims, out_dims))\n",
    "                module_list.append(GVP_(out_dims, out_dims, activations=(None, None)))\n",
    "        self.message_func = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"\n",
    "        :param x: tuple (s, V) of `torch.Tensor`\n",
    "        :param edge_index: array of shape [2, n_edges]\n",
    "        :param edge_attr: tuple (s, V) of `torch.Tensor`\n",
    "        \"\"\"\n",
    "        x_s, x_v = x\n",
    "        message = self.propagate(\n",
    "            edge_index,\n",
    "            s=x_s,\n",
    "            v=x_v.reshape(x_v.shape[0], 3 * x_v.shape[1]),\n",
    "            edge_attr=edge_attr,\n",
    "        )\n",
    "        return _split(message, self.vo)\n",
    "\n",
    "    def message(self, s_i, v_i, s_j, v_j, edge_attr):\n",
    "        v_j = v_j.view(v_j.shape[0], v_j.shape[1] // 3, 3)\n",
    "        v_i = v_i.view(v_i.shape[0], v_i.shape[1] // 3, 3)\n",
    "        message = tuple_cat((s_j, v_j), edge_attr, (s_i, v_i))\n",
    "        message = self.message_func(message)\n",
    "        return _merge(*message)\n",
    "\n",
    "\n",
    "class GVPConvLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Full graph convolution / message passing layer with\n",
    "    Geometric Vector Perceptrons. Residually updates node embeddings with\n",
    "    aggregated incoming messages, applies a pointwise feedforward\n",
    "    network to node embeddings, and returns updated node embeddings.\n",
    "\n",
    "    To only compute the aggregated messages, see `GVPConv`.\n",
    "\n",
    "    :param node_dims: node embedding dimensions (n_scalar, n_vector)\n",
    "    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)\n",
    "    :param n_message: number of GVPs to use in message function\n",
    "    :param n_feedforward: number of GVPs to use in feedforward function\n",
    "    :param drop_rate: drop probability in all dropout layers\n",
    "    :param autoregressive: if `True`, this `GVPConvLayer` will be used\n",
    "           with a different set of input node embeddings for messages\n",
    "           where src >= dst\n",
    "    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_dims,\n",
    "        edge_dims,\n",
    "        n_message=3,\n",
    "        n_feedforward=2,\n",
    "        drop_rate=0.1,\n",
    "        autoregressive=False,\n",
    "        activations=(F.relu, torch.sigmoid),\n",
    "        vector_gate=False,\n",
    "    ):\n",
    "\n",
    "        super(GVPConvLayer, self).__init__()\n",
    "        self.conv = GVPConv(\n",
    "            node_dims,\n",
    "            node_dims,\n",
    "            edge_dims,\n",
    "            n_message,\n",
    "            aggr=\"add\" if autoregressive else \"mean\",\n",
    "            activations=activations,\n",
    "            vector_gate=vector_gate,\n",
    "        )\n",
    "        GVP_ = functools.partial(GVP, activations=activations, vector_gate=vector_gate)\n",
    "        self.norm = nn.ModuleList([LayerNorm(node_dims) for _ in range(2)])\n",
    "        self.dropout = nn.ModuleList([Dropout(drop_rate) for _ in range(2)])\n",
    "\n",
    "        ff_func = []\n",
    "        if n_feedforward == 1:\n",
    "            ff_func.append(GVP_(node_dims, node_dims, activations=(None, None)))\n",
    "        else:\n",
    "            hid_dims = 4 * node_dims[0], 2 * node_dims[1]\n",
    "            ff_func.append(GVP_(node_dims, hid_dims))\n",
    "            for i in range(n_feedforward - 2):\n",
    "                ff_func.append(GVP_(hid_dims, hid_dims))\n",
    "            ff_func.append(GVP_(hid_dims, node_dims, activations=(None, None)))\n",
    "        self.ff_func = nn.Sequential(*ff_func)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, autoregressive_x=None, node_mask=None):\n",
    "        \"\"\"\n",
    "        :param x: tuple (s, V) of `torch.Tensor`\n",
    "        :param edge_index: array of shape [2, n_edges]\n",
    "        :param edge_attr: tuple (s, V) of `torch.Tensor`\n",
    "        :param autoregressive_x: tuple (s, V) of `torch.Tensor`.\n",
    "                If not `None`, will be used as src node embeddings\n",
    "                for forming messages where src >= dst. The corrent node\n",
    "                embeddings `x` will still be the base of the update and the\n",
    "                pointwise feedforward.\n",
    "        :param node_mask: array of type `bool` to index into the first\n",
    "                dim of node embeddings (s, V). If not `None`, only\n",
    "                these nodes will be updated.\n",
    "        \"\"\"\n",
    "\n",
    "        if autoregressive_x is not None:\n",
    "            src, dst = edge_index\n",
    "            mask = src < dst\n",
    "            edge_index_forward = edge_index[:, mask]\n",
    "            edge_index_backward = edge_index[:, ~mask]\n",
    "            edge_attr_forward = tuple_index(edge_attr, mask)\n",
    "            edge_attr_backward = tuple_index(edge_attr, ~mask)\n",
    "\n",
    "            dh = tuple_sum(\n",
    "                self.conv(x, edge_index_forward, edge_attr_forward),\n",
    "                self.conv(autoregressive_x, edge_index_backward, edge_attr_backward),\n",
    "            )\n",
    "\n",
    "            count = (\n",
    "                scatter_add(torch.ones_like(dst), dst, dim_size=dh[0].size(0))\n",
    "                .clamp(min=1)\n",
    "                .unsqueeze(-1)\n",
    "            )\n",
    "\n",
    "            dh = dh[0] / count, dh[1] / count.unsqueeze(-1)\n",
    "\n",
    "        else:\n",
    "            dh = self.conv(x, edge_index, edge_attr)\n",
    "\n",
    "        if node_mask is not None:\n",
    "            x_ = x\n",
    "            x, dh = tuple_index(x, node_mask), tuple_index(dh, node_mask)\n",
    "\n",
    "        x = self.norm[0](tuple_sum(x, self.dropout[0](dh)))\n",
    "\n",
    "        dh = self.ff_func(x)\n",
    "        x = self.norm[1](tuple_sum(x, self.dropout[1](dh)))\n",
    "\n",
    "        if node_mask is not None:\n",
    "            x_[0][node_mask], x_[1][node_mask] = x[0], x[1]\n",
    "            x = x_\n",
    "        return x\n",
    "\n",
    "\n",
    "class myResTransform(object):\n",
    "    # pos_oi is for example: [('A', 60, 'TRP'),('A', 61, 'ASP'), ('A', 64, 'LYS'), ('A', 80, 'GLU')]\n",
    "    # first entry is chain number, position in M1 indexing, and 3letter amino acid code\n",
    "\n",
    "    def __init__(self, balance=False, pos_oi=[]):\n",
    "        self.balance = balance\n",
    "        self.pos_oi = pos_oi\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x[\"id\"] = fi.get_pdb_code(x[\"id\"])\n",
    "        df = x[\"atoms\"]\n",
    "\n",
    "        subunits = []\n",
    "        # df = df.set_index(['chain', 'residue', 'resname'], drop=False)\n",
    "        df = df.dropna(subset=[\"x\", \"y\", \"z\"])\n",
    "        # remove Hets and non-allowable atoms\n",
    "        df = df[df[\"element\"].isin(allowed_atoms)]\n",
    "        df = df[df[\"hetero\"].str.strip() == \"\"]\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        labels = []\n",
    "\n",
    "        for chain_res, res_df in df.groupby([\"chain\", \"residue\", \"resname\"]):\n",
    "            # chain_res is something like ('A', 61, 'ASP')\n",
    "\n",
    "            if chain_res not in self.pos_oi:\n",
    "                continue\n",
    "            print(chain_res)  # s\n",
    "\n",
    "            chain, res, res_name = chain_res\n",
    "            # only train on canonical residues\n",
    "            if res_name not in res_label_dict:\n",
    "                continue\n",
    "            # sample each residue based on its frequency in train data\n",
    "            if self.balance:\n",
    "                if not np.random.random() < res_wt_dict[res_name]:\n",
    "                    continue\n",
    "\n",
    "            if not np.all([b in res_df[\"name\"].to_list() for b in bb_atoms]):\n",
    "                # print('residue missing atoms...   skipping')\n",
    "                continue\n",
    "            CA_pos = (\n",
    "                res_df[res_df[\"name\"] == \"CA\"][[\"x\", \"y\", \"z\"]]\n",
    "                .astype(np.float32)\n",
    "                .to_numpy()[0]\n",
    "            )\n",
    "\n",
    "            CB_pos = CA_pos + (np.ones_like(CA_pos) * gly_CB_mu)\n",
    "\n",
    "            # remove current residue from structure\n",
    "            subunit_df = df[\n",
    "                (df.chain != chain) | (df.residue != res) | df[\"name\"].isin(bb_atoms)\n",
    "            ]\n",
    "\n",
    "            # environment = all atoms within 10*sqrt(3) angstroms (to enable a 20A cube)\n",
    "            kd_tree = scipy.spatial.KDTree(subunit_df[[\"x\", \"y\", \"z\"]].to_numpy())\n",
    "            subunit_pt_idx = kd_tree.query_ball_point(\n",
    "                CB_pos, r=10.0 * np.sqrt(3), p=2.0\n",
    "            )\n",
    "\n",
    "            subunits.append(subunit_df.index[sorted(subunit_pt_idx)].to_list())\n",
    "\n",
    "            sub_name = \"_\".join([str(x) for x in chain_res])\n",
    "            label_row = [\n",
    "                sub_name,\n",
    "                res_label_dict[res_name],\n",
    "                CB_pos[0],\n",
    "                CB_pos[1],\n",
    "                CB_pos[2],\n",
    "            ]\n",
    "            labels.append(label_row)\n",
    "\n",
    "        assert len(labels) == len(subunits)\n",
    "        print(len(labels))\n",
    "        x[\"atoms\"] = df\n",
    "        x[\"labels\"] = pd.DataFrame(labels, columns=[\"subunit\", \"label\", \"x\", \"y\", \"z\"])\n",
    "        x[\"subunit_indices\"] = subunits\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A base 5-layer GVP-GNN for all ATOM3D tasks, using GVPs with\n",
    "    vector gating as described in the manuscript. Takes in atomic-level\n",
    "    structure graphs of type `torch_geometric.data.Batch`\n",
    "    and returns a single scalar.\n",
    "\n",
    "    This class should not be used directly. Instead, please use the\n",
    "    task-specific models which extend BaseModel. (Some of these classes\n",
    "    may be aliases of BaseModel.)\n",
    "\n",
    "    :param num_rbf: number of radial bases to use in the edge embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_rbf=16):\n",
    "\n",
    "        super().__init__()\n",
    "        activations = (F.relu, None)\n",
    "\n",
    "        self.embed = nn.Embedding(_NUM_ATOM_TYPES, _NUM_ATOM_TYPES)\n",
    "\n",
    "        self.W_e = nn.Sequential(\n",
    "            LayerNorm((num_rbf, 1)),\n",
    "            GVP(\n",
    "                (num_rbf, 1), _DEFAULT_E_DIM, activations=(None, None), vector_gate=True\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.W_v = nn.Sequential(\n",
    "            LayerNorm((_NUM_ATOM_TYPES, 0)),\n",
    "            GVP(\n",
    "                (_NUM_ATOM_TYPES, 0),\n",
    "                _DEFAULT_V_DIM,\n",
    "                activations=(None, None),\n",
    "                vector_gate=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            GVPConvLayer(\n",
    "                _DEFAULT_V_DIM,\n",
    "                _DEFAULT_E_DIM,\n",
    "                activations=activations,\n",
    "                vector_gate=True,\n",
    "            )\n",
    "            for _ in range(5)\n",
    "        )\n",
    "\n",
    "        ns, _ = _DEFAULT_V_DIM\n",
    "        self.W_out = nn.Sequential(\n",
    "            LayerNorm(_DEFAULT_V_DIM),\n",
    "            GVP(_DEFAULT_V_DIM, (ns, 0), activations=activations, vector_gate=True),\n",
    "        )\n",
    "\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(ns, 2 * ns),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(2 * ns, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch, scatter_mean=True, dense=True):\n",
    "        \"\"\"\n",
    "        Forward pass which can be adjusted based on task formulation.\n",
    "\n",
    "        :param batch: `torch_geometric.data.Batch` with data attributes\n",
    "                      as returned from a BaseTransform\n",
    "        :param scatter_mean: if `True`, returns mean of final node embeddings\n",
    "                             (for each graph), else, returns embeddings seperately\n",
    "        :param dense: if `True`, applies final dense layer to reduce embedding\n",
    "                      to a single scalar; else, returns the embedding\n",
    "        \"\"\"\n",
    "        h_V = self.embed(batch.atoms)\n",
    "        h_E = (batch.edge_s, batch.edge_v)\n",
    "        h_V = self.W_v(h_V)\n",
    "        h_E = self.W_e(h_E)\n",
    "\n",
    "        batch_id = batch.batch\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h_V = layer(h_V, batch.edge_index, h_E)\n",
    "\n",
    "        out = self.W_out(h_V)\n",
    "        if scatter_mean:\n",
    "            out = torch_scatter.scatter_mean(out, batch_id, dim=0)\n",
    "        if dense:\n",
    "            out = self.dense(out).squeeze(-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RESModel(BaseModel):\n",
    "    \"\"\"\n",
    "    GVP-GNN for the RES task.\n",
    "\n",
    "    Extends BaseModel to output a 20-dim vector instead of a single\n",
    "    scalar for each graph, which can be used as logits in 20-way\n",
    "    classification.\n",
    "\n",
    "    As noted in the manuscript, RESModel uses the final alpha\n",
    "    carbon embeddings instead of the graph mean embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ns, _ = _DEFAULT_V_DIM\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(ns, 2 * ns),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(2 * ns, 20),\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        out = super().forward(batch, scatter_mean=False)\n",
    "        return out[batch.ca_idx + batch.ptr[:-1]]\n",
    "\n",
    "\n",
    "def get_model(task):\n",
    "    return {\n",
    "        \"RES\": RESModel,\n",
    "    }[task]()\n",
    "\n",
    "\n",
    "def _normalize(tensor, dim=-1):\n",
    "    \"\"\"\n",
    "    Normalizes a `torch.Tensor` along dimension `dim` without `nan`s.\n",
    "    \"\"\"\n",
    "    return torch.nan_to_num(\n",
    "        torch.div(tensor, torch.norm(tensor, dim=dim, keepdim=True))\n",
    "    )\n",
    "\n",
    "\n",
    "def _rbf(D, D_min=0.0, D_max=20.0, D_count=16, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    From https://github.com/jingraham/neurips19-graph-protein-design\n",
    "\n",
    "    Returns an RBF embedding of `torch.Tensor` `D` along a new axis=-1.\n",
    "    That is, if `D` has shape [...dims], then the returned tensor will have\n",
    "    shape [...dims, D_count].\n",
    "    \"\"\"\n",
    "    D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n",
    "    D_mu = D_mu.view([1, -1])\n",
    "    D_sigma = (D_max - D_min) / D_count\n",
    "    D_expand = torch.unsqueeze(D, -1)\n",
    "\n",
    "    RBF = torch.exp(-(((D_expand - D_mu) / D_sigma) ** 2))\n",
    "    return RBF\n",
    "\n",
    "\n",
    "def _edge_features(coords, edge_index, D_max=4.5, num_rbf=16, device=\"cpu\"):\n",
    "\n",
    "    E_vectors = coords[edge_index[0]] - coords[edge_index[1]]\n",
    "    rbf = _rbf(E_vectors.norm(dim=-1), D_max=D_max, D_count=num_rbf, device=device)\n",
    "\n",
    "    edge_s = rbf\n",
    "    edge_v = _normalize(E_vectors).unsqueeze(-2)\n",
    "\n",
    "    edge_s, edge_v = map(torch.nan_to_num, (edge_s, edge_v))\n",
    "\n",
    "    return edge_s, edge_v\n",
    "\n",
    "\n",
    "class BaseTransform:\n",
    "    \"\"\"\n",
    "    Implementation of an ATOM3D Transform which featurizes the atomic\n",
    "    coordinates in an ATOM3D dataframes into `torch_geometric.data.Data`\n",
    "    graphs. This class should not be used directly; instead, use the\n",
    "    task-specific transforms, which all extend BaseTransform. Node\n",
    "    and edge features are as described in the EGNN manuscript.\n",
    "\n",
    "    Returned graphs have the following attributes:\n",
    "    -x          atomic coordinates, shape [n_nodes, 3]\n",
    "    -atoms      numeric encoding of atomic identity, shape [n_nodes]\n",
    "    -edge_index edge indices, shape [2, n_edges]\n",
    "    -edge_s     edge scalar features, shape [n_edges, 16]\n",
    "    -edge_v     edge scalar features, shape [n_edges, 1, 3]\n",
    "\n",
    "    Subclasses of BaseTransform will produce graphs with additional\n",
    "    attributes for the tasks-specific training labels, in addition\n",
    "    to the above.\n",
    "\n",
    "    All subclasses of BaseTransform directly inherit the BaseTransform\n",
    "    constructor.\n",
    "\n",
    "    :param edge_cutoff: distance cutoff to use when drawing edges\n",
    "    :param num_rbf: number of radial bases to encode the distance on each edge\n",
    "    :device: if \"cuda\", will do preprocessing on the GPU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, edge_cutoff=4.5, num_rbf=16, device=\"cpu\"):\n",
    "        self.edge_cutoff = edge_cutoff\n",
    "        self.num_rbf = num_rbf\n",
    "        self.device = device\n",
    "\n",
    "    def __call__(self, df):\n",
    "        \"\"\"\n",
    "        :param df: `pandas.DataFrame` of atomic coordinates\n",
    "                    in the ATOM3D format\n",
    "\n",
    "        :return: `torch_geometric.data.Data` structure graph\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            coords = torch.as_tensor(\n",
    "                df[[\"x\", \"y\", \"z\"]].to_numpy(), dtype=torch.float32, device=self.device\n",
    "            )\n",
    "            atoms = torch.as_tensor(\n",
    "                list(map(_element_mapping, df.element)),\n",
    "                dtype=torch.long,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "            edge_index = torch_cluster.radius_graph(coords, r=self.edge_cutoff)\n",
    "\n",
    "            edge_s, edge_v = _edge_features(\n",
    "                coords,\n",
    "                edge_index,\n",
    "                D_max=self.edge_cutoff,\n",
    "                num_rbf=self.num_rbf,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "            return torch_geometric.data.Data(\n",
    "                x=coords,\n",
    "                atoms=atoms,\n",
    "                edge_index=edge_index,\n",
    "                edge_s=edge_s,\n",
    "                edge_v=edge_v,\n",
    "            )\n",
    "\n",
    "\n",
    "class myRESDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    A `torch.utils.data.IterableDataset` wrapper around a\n",
    "    ATOM3D RES dataset.\n",
    "\n",
    "    On each iteration, returns a `torch_geometric.data.Data`\n",
    "    graph with the attribute `label` encoding the masked residue\n",
    "    identity, `ca_idx` for the node index of the alpha carbon,\n",
    "    and all structural attributes as described in BaseTransform.\n",
    "\n",
    "    Excludes hydrogen atoms.\n",
    "\n",
    "    :param lmdb_dataset: path to ATOM3D dataset\n",
    "    :param split_path: path to the ATOM3D split file\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lmdb_dataset, chain_id_oi=\"A\", split_path=None):\n",
    "        self.dataset = LMDBDataset(lmdb_dataset)  # load lmdb dataset as above\n",
    "        self.idx = [0]\n",
    "        self.transform = BaseTransform()\n",
    "        self.chain_id_oi = chain_id_oi\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            gen = self._dataset_generator(list(range(len(self.idx))), shuffle=False)\n",
    "        else:\n",
    "            per_worker = int(math.ceil(len(self.idx) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, len(self.idx))\n",
    "            gen = self._dataset_generator(\n",
    "                list(range(len(self.idx)))[iter_start:iter_end], shuffle=False\n",
    "            )\n",
    "        return gen\n",
    "\n",
    "    def _dataset_generator(self, indices, shuffle=False):\n",
    "        if shuffle:\n",
    "            random.shuffle(indices)\n",
    "        with torch.no_grad():\n",
    "            for idx in indices:\n",
    "                print(\"idx\", idx)\n",
    "                data = self.dataset[self.idx[idx]]\n",
    "                atoms = data[\"atoms\"]\n",
    "                for sub in data[\"labels\"].itertuples():\n",
    "                    _, num, aa_num = sub.subunit.split(\"_\")\n",
    "                    num, aa = int(num), _amino_acids(aa_num)\n",
    "                    if aa == 20:\n",
    "                        print(\"aais20\")\n",
    "                        continue\n",
    "                    my_atoms = atoms.iloc[\n",
    "                        data[\"subunit_indices\"][sub.Index]\n",
    "                    ].reset_index(drop=True)\n",
    "                    ca_idx = np.where(\n",
    "                        (my_atoms.residue == num)\n",
    "                        & (my_atoms.name == \"CA\")\n",
    "                        & (my_atoms.chain == self.chain_id_oi)\n",
    "                    )[\n",
    "                        0\n",
    "                    ]  # had to fix this\n",
    "                    if len(ca_idx) != 1:\n",
    "                        print(\"len(ca_idx) is not 1\")\n",
    "                        continue\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        graph = self.transform(my_atoms)\n",
    "                        graph.label = aa\n",
    "                        graph.ca_idx = int(ca_idx)\n",
    "                        yield num, aa, graph\n",
    "\n",
    "\n",
    "def forward(model, batch, device):\n",
    "    if type(batch) in [list, tuple]:\n",
    "        batch = batch[0].to(device), batch[1].to(device)\n",
    "    else:\n",
    "        batch = batch.to(device)\n",
    "    return model(batch)\n",
    "\n",
    "\n",
    "def get_gvp_res_prefs(\n",
    "    wt_seq,\n",
    "    protein_name,\n",
    "    chain_number,\n",
    "    lmdb_dout,\n",
    "    dout,\n",
    "    model_weight_path=\"/disk2/fli/ddingding-CoVES/data/coves/res_weights/RES_1646945484.3030427_8.pt\",\n",
    "    max_pos_to_do=1000,\n",
    "    n_ave=15,\n",
    "):\n",
    "\n",
    "    # uses RES GVP to calculate residue preferences from structural environment\n",
    "    # pdb_din: input directory of pdb file\n",
    "    # lmdb_dout: output directory for making lmdb file\n",
    "\n",
    "    ##############################################################################\n",
    "    # create list of positions that are of interest\n",
    "    pos_oi_all = list(\n",
    "        zip(\n",
    "            [chain_number] * len(wt_seq),\n",
    "            range(1, len(wt_seq) + 1),\n",
    "            [AA1_TO_AA3[aa] for aa in wt_seq],\n",
    "        )\n",
    "    )\n",
    "    # Load dataset from directory of PDB files\n",
    "    # this is recursive, all pdb files in subdirectories will also be used\n",
    "    # dataset = da.load_dataset(pdb_din, 'pdb',\n",
    "    #                           transform = myResTransform(balance=False, pos_oi =pos_oi_all))\n",
    "\n",
    "    # # Create LMDB dataset from PDB dataset, and write to file\n",
    "    # da.make_lmdb_dataset(dataset, lmdb_dout)\n",
    "\n",
    "    ########################## LOAD MODEL #######################################\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # push the model to cuda\n",
    "    model = get_model(\"RES\").to(device)\n",
    "\n",
    "    # load model\n",
    "    if device == \"cuda\":\n",
    "        model.load_state_dict(torch.load(model_weight_path))\n",
    "    else:\n",
    "        model.load_state_dict(\n",
    "            torch.load(model_weight_path, map_location=torch.device(\"cpu\"))\n",
    "        )\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    print(f\"Model loaded from {model_weight_path}\")\n",
    "\n",
    "    ds_all = myRESDataset(lmdb_dout, chain_id_oi=chain_number)\n",
    "    # dl_all = torch_geometric.data.DataLoader(ds_all, num_workers=4, batch_size=1)\n",
    "    dl_all = torch_geometric.loader.DataLoader(ds_all, num_workers=4, batch_size=1)\n",
    "\n",
    "    ########################## predicting mutation preferences ##################\n",
    "    df_result = pd.DataFrame()\n",
    "    with torch.no_grad():\n",
    "        c = 0\n",
    "        for d in tqdm.tqdm(dl_all):\n",
    "            num, aa, b = d\n",
    "            if c < max_pos_to_do:\n",
    "                pos = num.numpy()[0]\n",
    "                aa3 = num_to_aa3[aa.numpy()[0]]\n",
    "                x = np.zeros([n_ave, 20])\n",
    "                for i in range(n_ave):\n",
    "                    out = forward(model, b, device)\n",
    "                    m_out = out.cpu().detach().numpy().reshape(-1)\n",
    "\n",
    "                    x[i, :] = m_out\n",
    "\n",
    "                mean_x = x.mean(axis=0)\n",
    "                std_x = x.std(axis=0)\n",
    "\n",
    "                aa1 = AA3_TO_AA1[aa3]\n",
    "                wt_pos = aa1 + str(pos)\n",
    "\n",
    "                muts = [wt_pos + AA3_TO_AA1[k] for k in aa3_to_num.keys()]\n",
    "\n",
    "                zipped = list(zip(muts, mean_x, std_x))\n",
    "                df_pos = pd.DataFrame(zipped, columns=[\"mut\", \"mean_x\", \"std_x\"])\n",
    "\n",
    "                df_result = pd.concat([df_result, df_pos], axis=0)\n",
    "                c += 1\n",
    "                print(c)\n",
    "    df_result = df_result.reset_index()\n",
    "\n",
    "    df_path = checkNgen_folder(os.path.join(dout, str(n_ave)))\n",
    "    # print(df_result)\n",
    "    df_result.to_csv(\n",
    "        os.path.join(df_path, protein_name + \"_\" + chain_number + \".csv\"), index=False\n",
    "    )\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def run_coves(\n",
    "    protein_name: str,\n",
    "    data_dir: str = \"data\",\n",
    "    chain_number: str = \"A\",\n",
    "    lmdb_dir: str = \"lmdb\",\n",
    "    model_weight_path: str = \"/disk2/fli/ddingding-CoVES/data/coves/res_weights/RES_1646945484.3030427_8.pt\",\n",
    "    dout: str = \"coves\",\n",
    "    n_ave: int = 100,\n",
    "):\n",
    "    start = timeit.default_timer()\n",
    "    wt_seq = read_fasta(os.path.join(data_dir, protein_name, protein_name + \".fasta\"))\n",
    "\n",
    "    print(f\"Computing residue preferences for {len(wt_seq)} amino acids.\")\n",
    "    print(f\"Load lmdb from {os.path.join(lmdb_dir, protein_name)}\")\n",
    "\n",
    "    df_result = get_gvp_res_prefs(\n",
    "        wt_seq=wt_seq,\n",
    "        protein_name=protein_name,\n",
    "        chain_number=chain_number,\n",
    "        lmdb_dout=os.path.join(lmdb_dir, protein_name),\n",
    "        model_weight_path=model_weight_path,\n",
    "        dout=dout,\n",
    "        n_ave=n_ave,\n",
    "    )\n",
    "    end = timeit.default_timer()\n",
    "\n",
    "    print(\n",
    "        f\"Computing residue preferences for {len(wt_seq)} amino acids took {end-start} seconds.\"\n",
    "    )\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def run_all_coves(patern=\"lmdb/*\", n_ave=100):\n",
    "    for lmdb_path in glob(patern):\n",
    "        protein_name = get_file_name(lmdb_path)\n",
    "        print(f\"Running CoVES for {protein_name}\")\n",
    "        run_coves(protein_name, n_ave=n_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atom3d.datasets as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, functools\n",
    "from torch import nn, scatter_add\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_sum(*args):\n",
    "    '''\n",
    "    Sums any number of tuples (s, V) elementwise.\n",
    "    '''\n",
    "    return tuple(map(sum, zip(*args)))\n",
    "\n",
    "\n",
    "def tuple_cat(*args, dim=-1):\n",
    "    '''\n",
    "    Concatenates any number of tuples (s, V) elementwise.\n",
    "    \n",
    "    :param dim: dimension along which to concatenate when viewed\n",
    "                as the `dim` index for the scalar-channel tensors.\n",
    "                This means that `dim=-1` will be applied as\n",
    "                `dim=-2` for the vector-channel tensors.\n",
    "    '''\n",
    "    dim %= len(args[0][0].shape)\n",
    "    s_args, v_args = list(zip(*args))\n",
    "    return torch.cat(s_args, dim=dim), torch.cat(v_args, dim=dim)\n",
    "\n",
    "\n",
    "def tuple_index(x, idx):\n",
    "    '''\n",
    "    Indexes into a tuple (s, V) along the first dimension.\n",
    "    \n",
    "    :param idx: any object which can be used to index into a `torch.Tensor`\n",
    "    '''\n",
    "    return x[0][idx], x[1][idx]\n",
    "\n",
    "def _split(x, nv):\n",
    "    '''\n",
    "    Splits a merged representation of (s, V) back into a tuple. \n",
    "    Should be used only with `_merge(s, V)` and only if the tuple \n",
    "    representation cannot be used.\n",
    "    \n",
    "    :param x: the `torch.Tensor` returned from `_merge`\n",
    "    :param nv: the number of vector channels in the input to `_merge`\n",
    "    '''\n",
    "    v = torch.reshape(x[..., -3*nv:], x.shape[:-1] + (nv, 3))\n",
    "    s = x[..., :-3*nv]\n",
    "    return s, v\n",
    "\n",
    "def _merge(s, v):\n",
    "    '''\n",
    "    Merges a tuple (s, V) into a single `torch.Tensor`, where the\n",
    "    vector channels are flattened and appended to the scalar channels.\n",
    "    Should be used only if the tuple representation cannot be used.\n",
    "    Use `_split(x, nv)` to reverse.\n",
    "    '''\n",
    "    v = torch.reshape(v, v.shape[:-2] + (3*v.shape[-2],))\n",
    "    return torch.cat([s, v], -1)\n",
    "\n",
    "def _norm_no_nan(x, axis=-1, keepdims=False, eps=1e-8, sqrt=True):\n",
    "    '''\n",
    "    L2 norm of tensor clamped above a minimum value `eps`.\n",
    "    \n",
    "    :param sqrt: if `False`, returns the square of the L2 norm\n",
    "    '''\n",
    "    out = torch.clamp(torch.sum(torch.square(x), axis, keepdims), min=eps)\n",
    "    return torch.sqrt(out) if sqrt else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _VDropout(nn.Module):\n",
    "    '''\n",
    "    Vector channel dropout where the elements of each\n",
    "    vector channel are dropped together.\n",
    "    '''\n",
    "    def __init__(self, drop_rate):\n",
    "        super(_VDropout, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: `torch.Tensor` corresponding to vector channels\n",
    "        '''\n",
    "        device = self.dummy_param.device\n",
    "        if not self.training:\n",
    "            return x\n",
    "        mask = torch.bernoulli(\n",
    "            (1 - self.drop_rate) * torch.ones(x.shape[:-1], device=device)\n",
    "        ).unsqueeze(-1)\n",
    "        x = mask * x / (1 - self.drop_rate)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    '''\n",
    "    Combined LayerNorm for tuples (s, V).\n",
    "    Takes tuples (s, V) as input and as output.\n",
    "    '''\n",
    "    def __init__(self, dims):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.s, self.v = dims\n",
    "        self.scalar_norm = nn.LayerNorm(self.s)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`,\n",
    "                  or single `torch.Tensor` \n",
    "                  (will be assumed to be scalar channels)\n",
    "        '''\n",
    "        if not self.v:\n",
    "            return self.scalar_norm(x)\n",
    "        s, v = x\n",
    "        vn = _norm_no_nan(v, axis=-1, keepdims=True, sqrt=False)\n",
    "        vn = torch.sqrt(torch.mean(vn, dim=-2, keepdim=True))\n",
    "        return self.scalar_norm(s), v / vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Dropout(nn.Module):\n",
    "    '''\n",
    "    Combined dropout for tuples (s, V).\n",
    "    Takes tuples (s, V) as input and as output.\n",
    "    '''\n",
    "    def __init__(self, drop_rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.sdropout = nn.Dropout(drop_rate)\n",
    "        self.vdropout = _VDropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`,\n",
    "                  or single `torch.Tensor` \n",
    "                  (will be assumed to be scalar channels)\n",
    "        '''\n",
    "        if type(x) is torch.Tensor:\n",
    "            return self.sdropout(x)\n",
    "        s, v = x\n",
    "        return self.sdropout(s), self.vdropout(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GVP(nn.Module):\n",
    "    '''\n",
    "    Geometric Vector Perceptron. See manuscript and README.md\n",
    "    for more details.\n",
    "    \n",
    "    :param in_dims: tuple (n_scalar, n_vector)\n",
    "    :param out_dims: tuple (n_scalar, n_vector)\n",
    "    :param h_dim: intermediate number of vector channels, optional\n",
    "    :param activations: tuple of functions (scalar_act, vector_act)\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    '''\n",
    "    def __init__(self, in_dims, out_dims, h_dim=None,\n",
    "                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n",
    "        super(GVP, self).__init__()\n",
    "        self.si, self.vi = in_dims\n",
    "        self.so, self.vo = out_dims\n",
    "        self.vector_gate = vector_gate\n",
    "        if self.vi: \n",
    "            self.h_dim = h_dim or max(self.vi, self.vo) \n",
    "            self.wh = nn.Linear(self.vi, self.h_dim, bias=False)\n",
    "            self.ws = nn.Linear(self.h_dim + self.si, self.so)\n",
    "            if self.vo:\n",
    "                self.wv = nn.Linear(self.h_dim, self.vo, bias=False)\n",
    "                if self.vector_gate: self.wsv = nn.Linear(self.so, self.vo)\n",
    "        else:\n",
    "            self.ws = nn.Linear(self.si, self.so)\n",
    "        \n",
    "        self.scalar_act, self.vector_act = activations\n",
    "        self.dummy_param = nn.Parameter(torch.empty(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`, \n",
    "                  or (if vectors_in is 0), a single `torch.Tensor`\n",
    "        :return: tuple (s, V) of `torch.Tensor`,\n",
    "                 or (if vectors_out is 0), a single `torch.Tensor`\n",
    "        '''\n",
    "        if self.vi:\n",
    "            s, v = x\n",
    "            v = torch.transpose(v, -1, -2)\n",
    "            vh = self.wh(v)    \n",
    "            vn = _norm_no_nan(vh, axis=-2)\n",
    "            s = self.ws(torch.cat([s, vn], -1))\n",
    "            if self.vo: \n",
    "                v = self.wv(vh) \n",
    "                v = torch.transpose(v, -1, -2)\n",
    "                if self.vector_gate: \n",
    "                    if self.vector_act:\n",
    "                        gate = self.wsv(self.vector_act(s))\n",
    "                    else:\n",
    "                        gate = self.wsv(s)\n",
    "                    v = v * torch.sigmoid(gate).unsqueeze(-1)\n",
    "                elif self.vector_act:\n",
    "                    v = v * self.vector_act(\n",
    "                        _norm_no_nan(v, axis=-1, keepdims=True))\n",
    "        else:\n",
    "            s = self.ws(x)\n",
    "            if self.vo:\n",
    "                v = torch.zeros(s.shape[0], self.vo, 3,\n",
    "                                device=self.dummy_param.device)\n",
    "        if self.scalar_act:\n",
    "            s = self.scalar_act(s)\n",
    "        \n",
    "        return (s, v) if self.vo else s\n",
    "\n",
    "\n",
    "class GVPConv(MessagePassing):\n",
    "    '''\n",
    "    Graph convolution / message passing with Geometric Vector Perceptrons.\n",
    "    Takes in a graph with node and edge embeddings,\n",
    "    and returns new node embeddings.\n",
    "    \n",
    "    This does NOT do residual updates and pointwise feedforward layers\n",
    "    ---see `GVPConvLayer`.\n",
    "    \n",
    "    :param in_dims: input node embedding dimensions (n_scalar, n_vector)\n",
    "    :param out_dims: output node embedding dimensions (n_scalar, n_vector)\n",
    "    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)\n",
    "    :param n_layers: number of GVPs in the message function\n",
    "    :param module_list: preconstructed message function, overrides n_layers\n",
    "    :param aggr: should be \"add\" if some incoming edges are masked, as in\n",
    "                 a masked autoregressive decoder architecture, otherwise \"mean\"\n",
    "    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    '''\n",
    "    def __init__(self, in_dims, out_dims, edge_dims,\n",
    "                 n_layers=3, module_list=None, aggr=\"mean\", \n",
    "                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n",
    "        super(GVPConv, self).__init__(aggr=aggr)\n",
    "        self.si, self.vi = in_dims\n",
    "        self.so, self.vo = out_dims\n",
    "        self.se, self.ve = edge_dims\n",
    "        \n",
    "        GVP_ = functools.partial(GVP, \n",
    "                activations=activations, vector_gate=vector_gate)\n",
    "        \n",
    "        module_list = module_list or []\n",
    "        if not module_list:\n",
    "            if n_layers == 1:\n",
    "                module_list.append(\n",
    "                    GVP_((2*self.si + self.se, 2*self.vi + self.ve), \n",
    "                        (self.so, self.vo), activations=(None, None)))\n",
    "            else:\n",
    "                module_list.append(\n",
    "                    GVP_((2*self.si + self.se, 2*self.vi + self.ve), out_dims)\n",
    "                )\n",
    "                for i in range(n_layers - 2):\n",
    "                    module_list.append(GVP_(out_dims, out_dims))\n",
    "                module_list.append(GVP_(out_dims, out_dims,\n",
    "                                       activations=(None, None)))\n",
    "        self.message_func = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`\n",
    "        :param edge_index: array of shape [2, n_edges]\n",
    "        :param edge_attr: tuple (s, V) of `torch.Tensor`\n",
    "        '''\n",
    "        x_s, x_v = x\n",
    "        message = self.propagate(edge_index, \n",
    "                    s=x_s, v=x_v.reshape(x_v.shape[0], 3*x_v.shape[1]),\n",
    "                    edge_attr=edge_attr)\n",
    "        return _split(message, self.vo) \n",
    "\n",
    "    def message(self, s_i, v_i, s_j, v_j, edge_attr):\n",
    "        v_j = v_j.view(v_j.shape[0], v_j.shape[1]//3, 3)\n",
    "        v_i = v_i.view(v_i.shape[0], v_i.shape[1]//3, 3)\n",
    "        message = tuple_cat((s_j, v_j), edge_attr, (s_i, v_i))\n",
    "        message = self.message_func(message)\n",
    "        return _merge(*message)\n",
    "\n",
    "class GVPConvLayer(nn.Module):\n",
    "    '''\n",
    "    Full graph convolution / message passing layer with \n",
    "    Geometric Vector Perceptrons. Residually updates node embeddings with\n",
    "    aggregated incoming messages, applies a pointwise feedforward \n",
    "    network to node embeddings, and returns updated node embeddings.\n",
    "    \n",
    "    To only compute the aggregated messages, see `GVPConv`.\n",
    "    \n",
    "    :param node_dims: node embedding dimensions (n_scalar, n_vector)\n",
    "    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)\n",
    "    :param n_message: number of GVPs to use in message function\n",
    "    :param n_feedforward: number of GVPs to use in feedforward function\n",
    "    :param drop_rate: drop probability in all dropout layers\n",
    "    :param autoregressive: if `True`, this `GVPConvLayer` will be used\n",
    "           with a different set of input node embeddings for messages\n",
    "           where src >= dst\n",
    "    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs\n",
    "    :param vector_gate: whether to use vector gating.\n",
    "                        (vector_act will be used as sigma^+ in vector gating if `True`)\n",
    "    '''\n",
    "    def __init__(self, node_dims, edge_dims,\n",
    "                 n_message=3, n_feedforward=2, drop_rate=.1,\n",
    "                 autoregressive=False, \n",
    "                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n",
    "        \n",
    "        super(GVPConvLayer, self).__init__()\n",
    "        self.conv = GVPConv(node_dims, node_dims, edge_dims, n_message,\n",
    "                           aggr=\"add\" if autoregressive else \"mean\",\n",
    "                           activations=activations, vector_gate=vector_gate)\n",
    "        GVP_ = functools.partial(GVP, \n",
    "                activations=activations, vector_gate=vector_gate)\n",
    "        self.norm = nn.ModuleList([LayerNorm(node_dims) for _ in range(2)])\n",
    "        self.dropout = nn.ModuleList([Dropout(drop_rate) for _ in range(2)])\n",
    "\n",
    "        ff_func = []\n",
    "        if n_feedforward == 1:\n",
    "            ff_func.append(GVP_(node_dims, node_dims, activations=(None, None)))\n",
    "        else:\n",
    "            hid_dims = 4*node_dims[0], 2*node_dims[1]\n",
    "            ff_func.append(GVP_(node_dims, hid_dims))\n",
    "            for i in range(n_feedforward-2):\n",
    "                ff_func.append(GVP_(hid_dims, hid_dims))\n",
    "            ff_func.append(GVP_(hid_dims, node_dims, activations=(None, None)))\n",
    "        self.ff_func = nn.Sequential(*ff_func)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr,\n",
    "                autoregressive_x=None, node_mask=None):\n",
    "        '''\n",
    "        :param x: tuple (s, V) of `torch.Tensor`\n",
    "        :param edge_index: array of shape [2, n_edges]\n",
    "        :param edge_attr: tuple (s, V) of `torch.Tensor`\n",
    "        :param autoregressive_x: tuple (s, V) of `torch.Tensor`. \n",
    "                If not `None`, will be used as src node embeddings\n",
    "                for forming messages where src >= dst. The corrent node \n",
    "                embeddings `x` will still be the base of the update and the \n",
    "                pointwise feedforward.\n",
    "        :param node_mask: array of type `bool` to index into the first\n",
    "                dim of node embeddings (s, V). If not `None`, only\n",
    "                these nodes will be updated.\n",
    "        '''\n",
    "        \n",
    "        if autoregressive_x is not None:\n",
    "            src, dst = edge_index\n",
    "            mask = src < dst\n",
    "            edge_index_forward = edge_index[:, mask]\n",
    "            edge_index_backward = edge_index[:, ~mask]\n",
    "            edge_attr_forward = tuple_index(edge_attr, mask)\n",
    "            edge_attr_backward = tuple_index(edge_attr, ~mask)\n",
    "            \n",
    "            dh = tuple_sum(\n",
    "                self.conv(x, edge_index_forward, edge_attr_forward),\n",
    "                self.conv(autoregressive_x, edge_index_backward, edge_attr_backward)\n",
    "            )\n",
    "            \n",
    "            count = scatter_add(torch.ones_like(dst), dst,\n",
    "                        dim_size=dh[0].size(0)).clamp(min=1).unsqueeze(-1)\n",
    "            \n",
    "            dh = dh[0] / count, dh[1] / count.unsqueeze(-1)\n",
    "\n",
    "        else:\n",
    "            dh = self.conv(x, edge_index, edge_attr)\n",
    "        \n",
    "        if node_mask is not None:\n",
    "            x_ = x\n",
    "            x, dh = tuple_index(x, node_mask), tuple_index(dh, node_mask)\n",
    "            \n",
    "        x = self.norm[0](tuple_sum(x, self.dropout[0](dh)))\n",
    "        \n",
    "        dh = self.ff_func(x)\n",
    "        x = self.norm[1](tuple_sum(x, self.dropout[1](dh)))\n",
    "        \n",
    "        if node_mask is not None:\n",
    "            x_[0][node_mask], x_[1][node_mask] = x[0], x[1]\n",
    "            x = x_\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to go from 3 letter amino acid code to one letter amino acid code\n",
    "AA3_TO_AA1 = {\n",
    "    \"CYS\": \"C\",\n",
    "    \"ASP\": \"D\",\n",
    "    \"SER\": \"S\",\n",
    "    \"GLN\": \"Q\",\n",
    "    \"LYS\": \"K\",\n",
    "    \"ILE\": \"I\",\n",
    "    \"PRO\": \"P\",\n",
    "    \"THR\": \"T\",\n",
    "    \"PHE\": \"F\",\n",
    "    \"ASN\": \"N\",\n",
    "    \"GLY\": \"G\",\n",
    "    \"HIS\": \"H\",\n",
    "    \"LEU\": \"L\",\n",
    "    \"ARG\": \"R\",\n",
    "    \"TRP\": \"W\",\n",
    "    \"ALA\": \"A\",\n",
    "    \"VAL\": \"V\",\n",
    "    \"GLU\": \"E\",\n",
    "    \"TYR\": \"Y\",\n",
    "    \"MET\": \"M\",\n",
    "}\n",
    "\n",
    "AA1_TO_AA3 = dict(zip(AA3_TO_AA1.values(), AA3_TO_AA1.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "label_res_dict={0:'HIS',1:'LYS',2:'ARG',3:'ASP',4:'GLU',5:'SER',6:'THR',7:'ASN',8:'GLN',9:'ALA',10:'VAL',11:'LEU',12:'ILE',13:'MET',14:'PHE',15:'TYR',16:'TRP',17:'PRO',18:'GLY',19:'CYS'}\n",
    "res_label_dict={'HIS':0,'LYS':1,'ARG':2,'ASP':3,'GLU':4,'SER':5,'THR':6,'ASN':7,'GLN':8,'ALA':9,'VAL':10,'LEU':11,'ILE':12,'MET':13,'PHE':14,'TYR':15,'TRP':16,'PRO':17,'GLY':18,'CYS':19}\n",
    "bb_atoms = ['N', 'CA', 'C', 'O']\n",
    "allowed_atoms = ['C', 'O', 'N', 'S', 'P', 'SE']\n",
    "\n",
    "# computed statistics from training set\n",
    "res_wt_dict = {'HIS': 0.581391659111514, 'LYS': 0.266061611865989, 'ARG': 0.2796785729861747, 'ASP': 0.26563454667840314, 'GLU': 0.22814679094919596, 'SER': 0.2612916369563003, 'THR': 0.27832512315270935, 'ASN': 0.3477441570413752, 'GLN': 0.37781509139381086, 'ALA': 0.20421144813311043, 'VAL': 0.22354397064847012, 'LEU': 0.18395198072344454, 'ILE': 0.2631600545792168, 'MET': 0.6918305148744505, 'PHE': 0.3592224851905275, 'TYR': 0.4048964515721682, 'TRP': 0.9882874205355423, 'PRO': 0.32994186046511625, 'GLY': 0.2238561093317741, 'CYS': 1.0}\n",
    "\n",
    "gly_CB_mu = np.array([-0.5311191 , -0.75842446,  1.2198311 ], dtype=np.float32)\n",
    "gly_CB_sigma = np.array([[1.63731114e-03, 2.40018381e-04, 6.38361679e-04],\n",
    "       [2.40018381e-04, 6.87853419e-05, 1.43898267e-04],\n",
    "       [6.38361679e-04, 1.43898267e-04, 3.25022011e-04]], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atom3d.datasets import LMDBDataset\n",
    "import atom3d.datasets.datasets as da\n",
    "import atom3d.splits.splits as spl\n",
    "import atom3d.util.file as fi\n",
    "import atom3d.util.formats as fo\n",
    "from atom3d.util import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class myResTransform(object):\n",
    "    # pos_oi is for example: [('A', 60, 'TRP'),('A', 61, 'ASP'), ('A', 64, 'LYS'), ('A', 80, 'GLU')]\n",
    "    # first entry is chain number, position in M1 indexing, and 3letter amino acid code\n",
    "\n",
    "    def __init__(self, balance=False, pos_oi = []):\n",
    "        self.balance = balance\n",
    "        self.pos_oi = pos_oi\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x['id'] = fi.get_pdb_code(x['id'])\n",
    "        df = x['atoms']\n",
    "\n",
    "        subunits = []\n",
    "        # df = df.set_index(['chain', 'residue', 'resname'], drop=False)\n",
    "        df = df.dropna(subset=['x','y','z'])\n",
    "        #remove Hets and non-allowable atoms\n",
    "        df = df[df['element'].isin(allowed_atoms)]\n",
    "        df = df[df['hetero'].str.strip()=='']\n",
    "        df = df.reset_index(drop=True)\n",
    "        \n",
    "        labels = []\n",
    "\n",
    "        for chain_res, res_df in df.groupby(['chain', 'residue', 'resname']):\n",
    "            # chain_res is something like ('A', 61, 'ASP')\n",
    "\n",
    "            if chain_res not in self.pos_oi:\n",
    "                continue\n",
    "            print(chain_res)# s\n",
    "\n",
    "            chain, res, res_name = chain_res\n",
    "            # only train on canonical residues\n",
    "            if res_name not in res_label_dict:\n",
    "                continue\n",
    "            # sample each residue based on its frequency in train data\n",
    "            if self.balance:\n",
    "                if not np.random.random() < res_wt_dict[res_name]:\n",
    "                    continue\n",
    "\n",
    "            if not np.all([b in res_df['name'].to_list() for b in bb_atoms]):\n",
    "                # print('residue missing atoms...   skipping')\n",
    "                continue\n",
    "            CA_pos = res_df[res_df['name']=='CA'][['x', 'y', 'z']].astype(np.float32).to_numpy()[0]\n",
    "\n",
    "            CB_pos = CA_pos + (np.ones_like(CA_pos) * gly_CB_mu)\n",
    "\n",
    "            # remove current residue from structure\n",
    "            subunit_df = df[(df.chain != chain) | (df.residue != res) | df['name'].isin(bb_atoms)]\n",
    "            \n",
    "            # environment = all atoms within 10*sqrt(3) angstroms (to enable a 20A cube)\n",
    "            kd_tree = scipy.spatial.KDTree(subunit_df[['x','y','z']].to_numpy())\n",
    "            subunit_pt_idx = kd_tree.query_ball_point(CB_pos, r=10.0*np.sqrt(3), p=2.0)\n",
    "            \n",
    "            subunits.append(subunit_df.index[sorted(subunit_pt_idx)].to_list())\n",
    "    \n",
    "            sub_name = '_'.join([str(x) for x in chain_res])\n",
    "            label_row = [sub_name, res_label_dict[res_name], CB_pos[0], CB_pos[1], CB_pos[2]]\n",
    "            labels.append(label_row)\n",
    "\n",
    "        assert len(labels) == len(subunits)\n",
    "        print(len(labels))\n",
    "        x['atoms'] = df\n",
    "        x['labels'] = pd.DataFrame(labels, columns=['subunit', 'label', 'x', 'y', 'z'])\n",
    "        x['subunit_indices'] = subunits\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_NUM_ATOM_TYPES = 9\n",
    "_element_mapping = lambda x: {\n",
    "    'H' : 0,\n",
    "    'C' : 1,\n",
    "    'N' : 2,\n",
    "    'O' : 3,\n",
    "    'F' : 4,\n",
    "    'S' : 5,\n",
    "    'Cl': 6, 'CL': 6,\n",
    "    'P' : 7\n",
    "}.get(x, 8)\n",
    "\n",
    "_DEFAULT_V_DIM = (100, 16)\n",
    "_DEFAULT_E_DIM = (32, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseModel(nn.Module):\n",
    "    '''\n",
    "    A base 5-layer GVP-GNN for all ATOM3D tasks, using GVPs with \n",
    "    vector gating as described in the manuscript. Takes in atomic-level\n",
    "    structure graphs of type `torch_geometric.data.Batch`\n",
    "    and returns a single scalar.\n",
    "    \n",
    "    This class should not be used directly. Instead, please use the\n",
    "    task-specific models which extend BaseModel. (Some of these classes\n",
    "    may be aliases of BaseModel.)\n",
    "    \n",
    "    :param num_rbf: number of radial bases to use in the edge embedding\n",
    "    '''\n",
    "    def __init__(self, num_rbf=16):\n",
    "        \n",
    "        super().__init__()\n",
    "        activations = (F.relu, None)\n",
    "        \n",
    "        self.embed = nn.Embedding(_NUM_ATOM_TYPES, _NUM_ATOM_TYPES)\n",
    "        \n",
    "        self.W_e = nn.Sequential(\n",
    "            LayerNorm((num_rbf, 1)),\n",
    "            GVP((num_rbf, 1), _DEFAULT_E_DIM, \n",
    "                activations=(None, None), vector_gate=True)\n",
    "        )\n",
    "        \n",
    "        self.W_v = nn.Sequential(\n",
    "            LayerNorm((_NUM_ATOM_TYPES, 0)),\n",
    "            GVP((_NUM_ATOM_TYPES, 0), _DEFAULT_V_DIM,\n",
    "                activations=(None, None), vector_gate=True)\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "                GVPConvLayer(_DEFAULT_V_DIM, _DEFAULT_E_DIM, \n",
    "                             activations=activations, vector_gate=True) \n",
    "            for _ in range(5))\n",
    "        \n",
    "        ns, _ = _DEFAULT_V_DIM\n",
    "        self.W_out = nn.Sequential(\n",
    "            LayerNorm(_DEFAULT_V_DIM),\n",
    "            GVP(_DEFAULT_V_DIM, (ns, 0), \n",
    "                activations=activations, vector_gate=True)\n",
    "        )\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(ns, 2*ns), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(2*ns, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, batch, scatter_mean=True, dense=True):\n",
    "        '''\n",
    "        Forward pass which can be adjusted based on task formulation.\n",
    "        \n",
    "        :param batch: `torch_geometric.data.Batch` with data attributes\n",
    "                      as returned from a BaseTransform\n",
    "        :param scatter_mean: if `True`, returns mean of final node embeddings\n",
    "                             (for each graph), else, returns embeddings seperately\n",
    "        :param dense: if `True`, applies final dense layer to reduce embedding\n",
    "                      to a single scalar; else, returns the embedding\n",
    "        '''\n",
    "        h_V = self.embed(batch.atoms)\n",
    "        h_E = (batch.edge_s, batch.edge_v)\n",
    "        h_V = self.W_v(h_V)\n",
    "        h_E = self.W_e(h_E)\n",
    "        \n",
    "        batch_id = batch.batch\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            h_V = layer(h_V, batch.edge_index, h_E)\n",
    "\n",
    "        out = self.W_out(h_V)\n",
    "        if scatter_mean: out = torch_scatter.scatter_mean(out, batch_id, dim=0)\n",
    "        if dense: out = self.dense(out).squeeze(-1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        \n",
    "class RESModel(BaseModel):\n",
    "    '''\n",
    "    GVP-GNN for the RES task.\n",
    "    \n",
    "    Extends BaseModel to output a 20-dim vector instead of a single\n",
    "    scalar for each graph, which can be used as logits in 20-way\n",
    "    classification.\n",
    "    \n",
    "    As noted in the manuscript, RESModel uses the final alpha\n",
    "    carbon embeddings instead of the graph mean embedding.\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ns, _ = _DEFAULT_V_DIM\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(ns, 2*ns), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(2*ns, 20)\n",
    "        )\n",
    "    def forward(self, batch):\n",
    "        out = super().forward(batch, scatter_mean=False)\n",
    "        return out[batch.ca_idx+batch.ptr[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        \n",
    "def get_model(task):\n",
    "    return {\n",
    "        'RES' : RESModel,\n",
    "    }[task]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-cluster\n",
      "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy in /disk2/fli/miniconda3/envs/coves/lib/python3.9/site-packages (from torch-cluster) (1.13.0)\n",
      "Requirement already satisfied: numpy<2.3,>=1.22.4 in /disk2/fli/miniconda3/envs/coves/lib/python3.9/site-packages (from scipy->torch-cluster) (1.26.4)\n",
      "Building wheels for collected packages: torch-cluster\n",
      "  Building wheel for torch-cluster (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-cluster: filename=torch_cluster-1.6.3-cp39-cp39-linux_x86_64.whl size=724315 sha256=d2911020574fa264a8011204f343b264946c5ef45b172cc578fdcf24b5f1f056\n",
      "  Stored in directory: /disk2/fli/.cache/pip/wheels/fc/55/f3/d8a310d8f0c29653a7580789f3b53f15e3cd2e88ff8ed5eeb5\n",
      "Successfully built torch-cluster\n",
      "Installing collected packages: torch-cluster\n",
      "Successfully installed torch-cluster-1.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch-cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _normalize(tensor, dim=-1):\n",
    "    '''\n",
    "    Normalizes a `torch.Tensor` along dimension `dim` without `nan`s.\n",
    "    '''\n",
    "    return torch.nan_to_num(\n",
    "        torch.div(tensor, torch.norm(tensor, dim=dim, keepdim=True)))\n",
    "\n",
    "\n",
    "def _rbf(D, D_min=0., D_max=20., D_count=16, device='cpu'):\n",
    "    '''\n",
    "    From https://github.com/jingraham/neurips19-graph-protein-design\n",
    "    \n",
    "    Returns an RBF embedding of `torch.Tensor` `D` along a new axis=-1.\n",
    "    That is, if `D` has shape [...dims], then the returned tensor will have\n",
    "    shape [...dims, D_count].\n",
    "    '''\n",
    "    D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n",
    "    D_mu = D_mu.view([1, -1])\n",
    "    D_sigma = (D_max - D_min) / D_count\n",
    "    D_expand = torch.unsqueeze(D, -1)\n",
    "\n",
    "    RBF = torch.exp(-((D_expand - D_mu) / D_sigma) ** 2)\n",
    "    return RBF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _edge_features(coords, edge_index, D_max=4.5, num_rbf=16, device='cpu'):\n",
    "    \n",
    "    E_vectors = coords[edge_index[0]] - coords[edge_index[1]]\n",
    "    rbf = _rbf(E_vectors.norm(dim=-1), \n",
    "               D_max=D_max, D_count=num_rbf, device=device)\n",
    "\n",
    "    edge_s = rbf\n",
    "    edge_v = _normalize(E_vectors).unsqueeze(-2)\n",
    "\n",
    "    edge_s, edge_v = map(torch.nan_to_num,\n",
    "            (edge_s, edge_v))\n",
    "\n",
    "    return edge_s, edge_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTransform:\n",
    "    '''\n",
    "    Implementation of an ATOM3D Transform which featurizes the atomic\n",
    "    coordinates in an ATOM3D dataframes into `torch_geometric.data.Data`\n",
    "    graphs. This class should not be used directly; instead, use the\n",
    "    task-specific transforms, which all extend BaseTransform. Node\n",
    "    and edge features are as described in the EGNN manuscript.\n",
    "    \n",
    "    Returned graphs have the following attributes:\n",
    "    -x          atomic coordinates, shape [n_nodes, 3]\n",
    "    -atoms      numeric encoding of atomic identity, shape [n_nodes]\n",
    "    -edge_index edge indices, shape [2, n_edges]\n",
    "    -edge_s     edge scalar features, shape [n_edges, 16]\n",
    "    -edge_v     edge scalar features, shape [n_edges, 1, 3]\n",
    "    \n",
    "    Subclasses of BaseTransform will produce graphs with additional \n",
    "    attributes for the tasks-specific training labels, in addition \n",
    "    to the above.\n",
    "    \n",
    "    All subclasses of BaseTransform directly inherit the BaseTransform\n",
    "    constructor.\n",
    "    \n",
    "    :param edge_cutoff: distance cutoff to use when drawing edges\n",
    "    :param num_rbf: number of radial bases to encode the distance on each edge\n",
    "    :device: if \"cuda\", will do preprocessing on the GPU\n",
    "    '''\n",
    "    def __init__(self, edge_cutoff=4.5, num_rbf=16, device='cpu'):\n",
    "        self.edge_cutoff = edge_cutoff\n",
    "        self.num_rbf = num_rbf\n",
    "        self.device = device\n",
    "            \n",
    "    def __call__(self, df):\n",
    "        '''\n",
    "        :param df: `pandas.DataFrame` of atomic coordinates\n",
    "                    in the ATOM3D format\n",
    "        \n",
    "        :return: `torch_geometric.data.Data` structure graph\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            coords = torch.as_tensor(df[['x', 'y', 'z']].to_numpy(),\n",
    "                                     dtype=torch.float32, device=self.device)\n",
    "            atoms = torch.as_tensor(list(map(_element_mapping, df.element)),\n",
    "                                            dtype=torch.long, device=self.device)\n",
    "\n",
    "            edge_index = torch_cluster.radius_graph(coords, r=self.edge_cutoff)\n",
    "\n",
    "            edge_s, edge_v = _edge_features(coords, edge_index, \n",
    "                                D_max=self.edge_cutoff, num_rbf=self.num_rbf, device=self.device)\n",
    "\n",
    "            return torch_geometric.data.Data(x=coords, atoms=atoms,\n",
    "                        edge_index=edge_index, edge_s=edge_s, edge_v=edge_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import IterableDataset\n",
    "import torch, random, scipy, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_amino_acids = lambda x: {\n",
    "    'ALA': 0,\n",
    "    'ARG': 1,\n",
    "    'ASN': 2,\n",
    "    'ASP': 3,\n",
    "    'CYS': 4,\n",
    "    'GLU': 5,\n",
    "    'GLN': 6,\n",
    "    'GLY': 7,\n",
    "    'HIS': 8,\n",
    "    'ILE': 9,\n",
    "    'LEU': 10,\n",
    "    'LYS': 11,\n",
    "    'MET': 12,\n",
    "    'PHE': 13,\n",
    "    'PRO': 14,\n",
    "    'SER': 15,\n",
    "    'THR': 16,\n",
    "    'TRP': 17,\n",
    "    'TYR': 18,\n",
    "    'VAL': 19\n",
    "}.get(x, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class myRESDataset(IterableDataset):\n",
    "    '''\n",
    "    A `torch.utils.data.IterableDataset` wrapper around a\n",
    "    ATOM3D RES dataset.\n",
    "    \n",
    "    On each iteration, returns a `torch_geometric.data.Data`\n",
    "    graph with the attribute `label` encoding the masked residue\n",
    "    identity, `ca_idx` for the node index of the alpha carbon, \n",
    "    and all structural attributes as described in BaseTransform.\n",
    "    \n",
    "    Excludes hydrogen atoms.\n",
    "    \n",
    "    :param lmdb_dataset: path to ATOM3D dataset\n",
    "    :param split_path: path to the ATOM3D split file\n",
    "    '''\n",
    "    def __init__(self, lmdb_dataset, chain_id_oi = 'A',split_path=None):\n",
    "        self.dataset = LMDBDataset(lmdb_dataset) #load lmdb dataset as above\n",
    "        self.idx = [0]\n",
    "        self.transform = BaseTransform()\n",
    "        self.chain_id_oi = chain_id_oi\n",
    "        \n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:\n",
    "            gen = self._dataset_generator(list(range(len(self.idx))), \n",
    "                      shuffle=False)\n",
    "        else:  \n",
    "            per_worker = int(math.ceil(len(self.idx) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, len(self.idx))\n",
    "            gen = self._dataset_generator(list(range(len(self.idx)))[iter_start:iter_end],\n",
    "                      shuffle=False)\n",
    "        return gen\n",
    "    \n",
    "    def _dataset_generator(self, indices, shuffle=False):\n",
    "        if shuffle: random.shuffle(indices)\n",
    "        with torch.no_grad():\n",
    "            for idx in indices:\n",
    "                print('idx',idx)\n",
    "                data = self.dataset[self.idx[idx]]\n",
    "                atoms = data['atoms']\n",
    "                for sub in data['labels'].itertuples():\n",
    "                    _, num, aa_num = sub.subunit.split('_')\n",
    "                    num, aa = int(num), _amino_acids(aa_num)\n",
    "                    if aa == 20: \n",
    "                        print('aais20')\n",
    "                        continue\n",
    "                    my_atoms = atoms.iloc[data['subunit_indices'][sub.Index]].reset_index(drop=True)\n",
    "                    ca_idx = np.where((my_atoms.residue == num) & (my_atoms.name == 'CA') &(my_atoms.chain  ==self.chain_id_oi))[0] # had to fix this\n",
    "                    if len(ca_idx) != 1: \n",
    "                        print('len(ca_idx) is not 1')\n",
    "                        continue\n",
    "                        \n",
    "                    with torch.no_grad():\n",
    "                        graph = self.transform(my_atoms)\n",
    "                        graph.label = aa\n",
    "                        graph.ca_idx = int(ca_idx)\n",
    "                        yield num, aa, graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aa3_to_num = {\n",
    "    'ALA': 0,\n",
    "    'ARG': 1,\n",
    "    'ASN': 2,\n",
    "    'ASP': 3,\n",
    "    'CYS': 4,\n",
    "    'GLU': 5,\n",
    "    'GLN': 6,\n",
    "    'GLY': 7,\n",
    "    'HIS': 8,\n",
    "    'ILE': 9,\n",
    "    'LEU': 10,\n",
    "    'LYS': 11,\n",
    "    'MET': 12,\n",
    "    'PHE': 13,\n",
    "    'PRO': 14,\n",
    "    'SER': 15,\n",
    "    'THR': 16,\n",
    "    'TRP': 17,\n",
    "    'TYR': 18,\n",
    "    'VAL': 19\n",
    "}\n",
    "\n",
    "\n",
    "num_to_aa3 = dict(zip(aa3_to_num.values(), aa3_to_num.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, batch, device):\n",
    "    if type(batch) in [list, tuple]:\n",
    "        batch = batch[0].to(device), batch[1].to(device)\n",
    "    else:\n",
    "        batch = batch.to(device)\n",
    "    return model(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_gvp_res_prefs(wt_seq='',\n",
    "                      protein_name ='protein',\n",
    "                     chain_number='',\n",
    "                     pdb_din='',\n",
    "                     lmdb_dout=\"\",\n",
    "                      model_weight_path = '/disk2/fli/ddingding-CoVES/data/coves/res_weights/RES_1646945484.3030427_8.pt',\n",
    "                      dout = './', \n",
    "                      max_pos_to_do = 1000,\n",
    "                      n_ave = 15\n",
    "                     ):\n",
    "    \n",
    "    # uses RES GVP to calculate residue preferences from structural environment\n",
    "    # pdb_din: input directory of pdb file\n",
    "    # lmdb_dout: output directory for making lmdb file\n",
    "\n",
    "    ##############################################################################\n",
    "    # create list of positions that are of interest\n",
    "    pos_oi_all = list(zip([chain_number]* len(wt_seq),\n",
    "                         range(1,len(wt_seq)+1),\n",
    "                         [AA1_TO_AA3[aa] for aa in wt_seq]\n",
    "                        )\n",
    "                    )\n",
    "    # Load dataset from directory of PDB files \n",
    "    # this is recursive, all pdb files in subdirectories will also be used\n",
    "    # dataset = da.load_dataset(pdb_din, 'pdb', \n",
    "    #                           transform = myResTransform(balance=False, pos_oi =pos_oi_all)) \n",
    "\n",
    "    # # Create LMDB dataset from PDB dataset, and write to file\n",
    "    # da.make_lmdb_dataset(dataset, lmdb_dout)\n",
    "    \n",
    "    ########################## LOAD MODEL #######################################\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # push the model to cuda \n",
    "    model = get_model('RES').to(device)\n",
    "\n",
    "    #load model\n",
    "    if device == 'cuda':\n",
    "        model.load_state_dict(torch.load(model_weight_path))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(model_weight_path, map_location=torch.device('cpu')))\n",
    "\n",
    "    model = model.eval()\n",
    "    \n",
    "    ds_all = myRESDataset(lmdb_dout, chain_id_oi=chain_number)\n",
    "    # dl_all = torch_geometric.data.DataLoader(ds_all, num_workers=4, batch_size=1)\n",
    "    dl_all = torch_geometric.loader.DataLoader(ds_all, num_workers=4, batch_size=1)\n",
    "\n",
    "    ########################## predicting mutation preferences ##################\n",
    "    df_result = pd.DataFrame()\n",
    "    with torch.no_grad():\n",
    "        c=0\n",
    "        for d in tqdm.tqdm(dl_all):\n",
    "            num, aa, b = d\n",
    "            if c<max_pos_to_do:\n",
    "                pos = num.numpy()[0]\n",
    "                aa3 = num_to_aa3[aa.numpy()[0]]\n",
    "                x= np.zeros([n_ave, 20])\n",
    "                for i in range(n_ave):\n",
    "                    out = forward(model, b, device)\n",
    "                    m_out= out.cpu().detach().numpy().reshape(-1)\n",
    "\n",
    "                    x[i,:] = m_out\n",
    "\n",
    "                mean_x = x.mean(axis=0)\n",
    "                std_x = x.std(axis=0)\n",
    "\n",
    "                aa1 = AA3_TO_AA1[aa3]\n",
    "                wt_pos = aa1+str(pos)\n",
    "\n",
    "                muts = [wt_pos+AA3_TO_AA1[k] for k in aa3_to_num.keys()]\n",
    "\n",
    "                zipped = list(zip(muts, mean_x, std_x))\n",
    "                df_pos = pd.DataFrame(zipped, columns=['mut', 'mean_x', 'std_x'])\n",
    "\n",
    "                df_result = pd.concat([df_result,df_pos], axis=0)\n",
    "                c+=1\n",
    "                print(c)\n",
    "    df_result = df_result.reset_index()\n",
    "    #print(df_result)\n",
    "    df_result.to_csv(dout+'gvp_{}_m_{}_230523_.csv'.format(n_ave, protein_name))\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the antitoxin residue preferences from the strutural surrounding\n",
    "wt_at = 'MANVEKMSVAVTPQQAAVMREAVEAGEYATASEIVREAVRDWLAKRELRHDDIRRLRQLWDEGKASGRPEPVDFDALRKEARQKLTEVPPNGR'\n",
    "n_ave = 100\n",
    "\n",
    "start = timeit.default_timer()\n",
    "df_result = get_gvp_res_prefs(wt_seq=wt_at,\n",
    "                                protein_name ='at',\n",
    "                                chain_number='A',\n",
    "                                # pdb_din=pdb_dir+'at/ta/',\n",
    "                                # lmdb_dout=pdb_dir+'at/lmdb_at_all/',\n",
    "                                # lmdb_dout = \"/disk2/fli/SSMuLA/lmdb\",\n",
    "                                lmdb_dout = \"/disk2/fli/ddingding-CoVES/data/coves/pdbs/pabp/lmdb\",\n",
    "                                model_weight_path = \"/disk2/fli/ddingding-CoVES/data/coves/res_weights/RES_1646945484.3030427_8.pt\",\n",
    "                                dout = \"/disk2/fli/SSMuLA/coves\", \n",
    "                                n_ave = n_ave\n",
    "                             )\n",
    "end = timeit.default_timer()\n",
    "\n",
    "print(f'Computing residue preferences for {len(wt_at)} amino acids took {end-start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>mut</th>\n",
       "      <th>mean_x</th>\n",
       "      <th>std_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A11A</td>\n",
       "      <td>-6.286508</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A11R</td>\n",
       "      <td>-6.669939</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A11N</td>\n",
       "      <td>-5.280306</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A11D</td>\n",
       "      <td>-6.569097</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A11C</td>\n",
       "      <td>-7.270478</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>15</td>\n",
       "      <td>R179S</td>\n",
       "      <td>-4.541832</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>16</td>\n",
       "      <td>R179T</td>\n",
       "      <td>-3.489768</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>17</td>\n",
       "      <td>R179W</td>\n",
       "      <td>-2.647094</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>18</td>\n",
       "      <td>R179Y</td>\n",
       "      <td>-2.080436</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>19</td>\n",
       "      <td>R179V</td>\n",
       "      <td>-3.154127</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3380 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index    mut    mean_x  std_x\n",
       "0         0   A11A -6.286508    0.0\n",
       "1         1   A11R -6.669939    0.0\n",
       "2         2   A11N -5.280306    0.0\n",
       "3         3   A11D -6.569097    0.0\n",
       "4         4   A11C -7.270478    0.0\n",
       "...     ...    ...       ...    ...\n",
       "3375     15  R179S -4.541832    0.0\n",
       "3376     16  R179T -3.489768    0.0\n",
       "3377     17  R179W -2.647094    0.0\n",
       "3378     18  R179Y -2.080436    0.0\n",
       "3379     19  R179V -3.154127    0.0\n",
       "\n",
       "[3380 rows x 4 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.to_csv(\"/disk2/fli/SSMuLA/coves/ParD3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1379326/190940212.py:993: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  graph.ca_idx = int(ca_idx)\n",
      "/tmp/ipykernel_1379326/190940212.py:993: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  graph.ca_idx = int(ca_idx)\n",
      "/tmp/ipykernel_1379326/190940212.py:993: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  graph.ca_idx = int(ca_idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11]) tensor([0]) DataBatch(x=[438, 3], edge_index=[2, 6692], atoms=[438], edge_s=[6692, 16], edge_v=[6692, 1, 3], label=[1], ca_idx=[1], batch=[438], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "ds_all = myRESDataset(lmdb_dataset=\"/disk2/fli/ddingding-CoVES/data/coves/pdbs/pabp/lmdb\", chain_id_oi=\"A\")\n",
    "dl_all = torch_geometric.loader.DataLoader(ds_all, num_workers=4, batch_size=1)\n",
    "for d in dl_all:\n",
    "    num, aa, b = d\n",
    "    print(num, aa, b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_oi_all=[('A', 1, 'MET'), ('A', 2, 'ALA'), ('A', 3, 'ASN'), ('A', 4, 'VAL'), ('A', 5, 'GLU'), ('A', 6, 'LYS'), ('A', 7, 'MET'), ('A', 8, 'SER'), ('A', 9, 'VAL'), ('A', 10, 'ALA'), ('A', 11, 'VAL'), ('A', 12, 'THR'), ('A', 13, 'PRO'), ('A', 14, 'GLN'), ('A', 15, 'GLN'), ('A', 16, 'ALA'), ('A', 17, 'ALA'), ('A', 18, 'VAL'), ('A', 19, 'MET'), ('A', 20, 'ARG'), ('A', 21, 'GLU'), ('A', 22, 'ALA'), ('A', 23, 'VAL'), ('A', 24, 'GLU'), ('A', 25, 'ALA'), ('A', 26, 'GLY'), ('A', 27, 'GLU'), ('A', 28, 'TYR'), ('A', 29, 'ALA'), ('A', 30, 'THR'), ('A', 31, 'ALA'), ('A', 32, 'SER'), ('A', 33, 'GLU'), ('A', 34, 'ILE'), ('A', 35, 'VAL'), ('A', 36, 'ARG'), ('A', 37, 'GLU'), ('A', 38, 'ALA'), ('A', 39, 'VAL'), ('A', 40, 'ARG'), ('A', 41, 'ASP'), ('A', 42, 'TRP'), ('A', 43, 'LEU'), ('A', 44, 'ALA'), ('A', 45, 'LYS'), ('A', 46, 'ARG'), ('A', 47, 'GLU'), ('A', 48, 'LEU'), ('A', 49, 'ARG'), ('A', 50, 'GLU'), ('A', 51, 'ALA'), ('A', 52, 'GLU'), ('A', 53, 'ALA'), ('A', 54, 'GLU'), ('A', 55, 'ARG'), ('A', 56, 'LEU'), ('A', 57, 'ARG'), ('A', 58, 'LYS'), ('A', 59, 'ALA'), ('A', 60, 'TRP'), ('A', 61, 'ILE'), ('A', 62, 'GLU'), ('A', 63, 'GLY'), ('A', 64, 'LEU'), ('A', 65, 'GLU'), ('A', 66, 'SER'), ('A', 67, 'GLY'), ('A', 68, 'PRO'), ('A', 69, 'PHE'), ('A', 70, 'ALA'), ('A', 71, 'PRO'), ('A', 72, 'PHE'), ('A', 73, 'ASP'), ('A', 74, 'ILE'), ('A', 75, 'GLU'), ('A', 76, 'ASP'), ('A', 77, 'ILE'), ('A', 78, 'LYS'), ('A', 79, 'GLN'), ('A', 80, 'LYS'), ('A', 81, 'ALA'), ('A', 82, 'ARG'), ('A', 83, 'SER'), ('A', 84, 'ARG'), ('A', 85, 'LEU'), ('A', 86, 'VAL'), ('A', 87, 'ASP'), ('A', 88, 'ALA'), ('A', 89, 'ILE'), ('A', 90, 'LYS'), ('A', 91, 'LYS')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = da.load_dataset(\"/disk2/fli/SSMuLA/coves_data/ParD2\", 'pdb', transform=myResTransform(balance=False, pos_oi=pos_oi_all))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines read from file: 1381\n"
     ]
    }
   ],
   "source": [
    "# Check if the PDB file can be read directly in Python\n",
    "try:\n",
    "    with open(\"data/ParD2/ParD2.pdb\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    print(f'Total lines read from file: {len(lines)}')\n",
    "except Exception as e:\n",
    "    print(f'Failed to read file: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in PDB dataset: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of entries in PDB dataset: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', 3, 'ASN')\n",
      "('A', 4, 'VAL')\n",
      "('A', 5, 'GLU')\n",
      "('A', 6, 'LYS')\n",
      "('A', 7, 'MET')\n",
      "('A', 8, 'SER')\n",
      "('A', 9, 'VAL')\n",
      "('A', 10, 'ALA')\n",
      "('A', 11, 'VAL')\n",
      "('A', 12, 'THR')\n",
      "('A', 13, 'PRO')\n",
      "('A', 14, 'GLN')\n",
      "('A', 15, 'GLN')\n",
      "('A', 16, 'ALA')\n",
      "('A', 17, 'ALA')\n",
      "('A', 18, 'VAL')\n",
      "('A', 19, 'MET')\n",
      "('A', 20, 'ARG')\n",
      "('A', 21, 'GLU')\n",
      "('A', 22, 'ALA')\n",
      "('A', 23, 'VAL')\n",
      "('A', 24, 'GLU')\n",
      "('A', 25, 'ALA')\n",
      "('A', 26, 'GLY')\n",
      "('A', 27, 'GLU')\n",
      "('A', 28, 'TYR')\n",
      "('A', 29, 'ALA')\n",
      "('A', 30, 'THR')\n",
      "('A', 31, 'ALA')\n",
      "('A', 32, 'SER')\n",
      "('A', 33, 'GLU')\n",
      "('A', 34, 'ILE')\n",
      "('A', 35, 'VAL')\n",
      "('A', 36, 'ARG')\n",
      "('A', 37, 'GLU')\n",
      "('A', 38, 'ALA')\n",
      "('A', 39, 'VAL')\n",
      "('A', 40, 'ARG')\n",
      "('A', 41, 'ASP')\n",
      "('A', 42, 'TRP')\n",
      "('A', 43, 'LEU')\n",
      "('A', 44, 'ALA')\n",
      "('A', 45, 'LYS')\n",
      "('A', 46, 'ARG')\n",
      "('A', 47, 'GLU')\n",
      "('A', 48, 'LEU')\n",
      "('A', 49, 'ARG')\n",
      "('A', 50, 'GLU')\n",
      "('A', 51, 'ALA')\n",
      "('A', 52, 'GLU')\n",
      "('A', 53, 'ALA')\n",
      "('A', 54, 'GLU')\n",
      "('A', 55, 'ARG')\n",
      "('A', 56, 'LEU')\n",
      "('A', 57, 'ARG')\n",
      "('A', 58, 'LYS')\n",
      "('A', 59, 'ALA')\n",
      "('A', 60, 'TRP')\n",
      "('A', 61, 'ILE')\n",
      "('A', 62, 'GLU')\n",
      "('A', 63, 'GLY')\n",
      "('A', 64, 'LEU')\n",
      "('A', 65, 'GLU')\n",
      "('A', 66, 'SER')\n",
      "('A', 67, 'GLY')\n",
      "('A', 68, 'PRO')\n",
      "('A', 69, 'PHE')\n",
      "('A', 70, 'ALA')\n",
      "('A', 71, 'PRO')\n",
      "('A', 72, 'PHE')\n",
      "('A', 73, 'ASP')\n",
      "('A', 74, 'ILE')\n",
      "('A', 75, 'GLU')\n",
      "('A', 76, 'ASP')\n",
      "('A', 77, 'ILE')\n",
      "('A', 78, 'LYS')\n",
      "('A', 79, 'GLN')\n",
      "('A', 80, 'LYS')\n",
      "('A', 81, 'ALA')\n",
      "('A', 82, 'ARG')\n",
      "('A', 83, 'SER')\n",
      "('A', 84, 'ARG')\n",
      "('A', 85, 'LEU')\n",
      "('A', 86, 'VAL')\n",
      "('A', 87, 'ASP')\n",
      "('A', 88, 'ALA')\n",
      "('A', 89, 'ILE')\n",
      "87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "da.make_lmdb_dataset(dataset, \"/disk2/fli/SSMuLA/lmdb/ParD2\")\n",
    "# lmdb_dataset = da.LMDBDataset(lmdb_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/disk2/fli/miniconda3/envs/coves/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/disk2/fli/miniconda3/envs/coves/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n    data.append(next(self.dataset_iter))\n  File \"/tmp/ipykernel_1379326/190940212.py\", line 968, in _dataset_generator\n    data = self.dataset[self.idx[idx]]\n  File \"/disk2/fli/miniconda3/envs/coves/lib/python3.9/site-packages/atom3d/datasets/datasets.py\", line 92, in __getitem__\n    raise IndexError(index)\nIndexError: 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_all \u001b[38;5;241m=\u001b[39m myRESDataset(lmdb_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/disk2/fli/SSMuLA/lmdb/ParD2\u001b[39m\u001b[38;5;124m\"\u001b[39m, chain_id_oi\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m dl_all \u001b[38;5;241m=\u001b[39m torch_geometric\u001b[38;5;241m.\u001b[39mloader\u001b[38;5;241m.\u001b[39mDataLoader(ds_all, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dl_all:\n\u001b[1;32m      4\u001b[0m     num, aa, b \u001b[38;5;241m=\u001b[39m d\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(num, aa, b)\n",
      "File \u001b[0;32m~/miniconda3/envs/coves/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/coves/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/coves/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/coves/lib/python3.9/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/disk2/fli/miniconda3/envs/coves/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/disk2/fli/miniconda3/envs/coves/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n    data.append(next(self.dataset_iter))\n  File \"/tmp/ipykernel_1379326/190940212.py\", line 968, in _dataset_generator\n    data = self.dataset[self.idx[idx]]\n  File \"/disk2/fli/miniconda3/envs/coves/lib/python3.9/site-packages/atom3d/datasets/datasets.py\", line 92, in __getitem__\n    raise IndexError(index)\nIndexError: 0\n"
     ]
    }
   ],
   "source": [
    "ds_all = myRESDataset(lmdb_dataset=\"/disk2/fli/SSMuLA/lmdb/ParD2\", chain_id_oi=\"A\")\n",
    "dl_all = torch_geometric.loader.DataLoader(ds_all, num_workers=4, batch_size=1)\n",
    "for d in dl_all:\n",
    "    num, aa, b = d\n",
    "    print(num, aa, b)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_geometric.loader.dataloader.DataLoader at 0x7f9ca4077f40>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1997758/2728505039.py:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  graph.ca_idx = int(ca_idx)\n",
      "/tmp/ipykernel_1997758/2728505039.py:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  graph.ca_idx = int(ca_idx)\n",
      "/tmp/ipykernel_1997758/2728505039.py:58: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  graph.ca_idx = int(ca_idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11]) tensor([0]) DataBatch(x=[438, 3], edge_index=[2, 6692], atoms=[438], edge_s=[6692, 16], edge_v=[6692, 1, 3], label=[1], ca_idx=[1], batch=[438], ptr=[2])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from glob import glob\n",
    "import tqdm\n",
    "\n",
    "import timeit\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import functools\n",
    "import torch\n",
    "from torch import nn, scatter_add\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_cluster\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "\n",
    "from atom3d.datasets import LMDBDataset\n",
    "import atom3d.datasets.datasets as da\n",
    "import atom3d.splits.splits as spl\n",
    "import atom3d.util.file as fi\n",
    "import atom3d.util.formats as fo\n",
    "from atom3d.util import metrics\n",
    "\n",
    "# from SSMuLA.util import checkNgen_folder, get_file_name, read_fasta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_din = \"data/TEV/TEV.pdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_oi_all = [('A', 1, 'MET'), ('A', 2, 'PHE'), ('A', 3, 'LYS'), ('A', 4, 'GLY'), ('A', 5, 'PRO'), ('A', 6, 'ARG'), ('A', 7, 'ASP'), ('A', 8, 'TYR'), ('A', 9, 'ASN'), ('A', 10, 'PRO'), ('A', 11, 'ILE'), ('A', 12, 'SER'), ('A', 13, 'SER'), ('A', 14, 'THR'), ('A', 15, 'ILE'), ('A', 16, 'CYS'), ('A', 17, 'HIS'), ('A', 18, 'LEU'), ('A', 19, 'THR'), ('A', 20, 'ASN'), ('A', 21, 'GLU'), ('A', 22, 'SER'), ('A', 23, 'ASP'), ('A', 24, 'GLY'), ('A', 25, 'HIS'), ('A', 26, 'THR'), ('A', 27, 'THR'), ('A', 28, 'SER'), ('A', 29, 'LEU'), ('A', 30, 'TYR'), ('A', 31, 'GLY'), ('A', 32, 'ILE'), ('A', 33, 'GLY'), ('A', 34, 'PHE'), ('A', 35, 'GLY'), ('A', 36, 'PRO'), ('A', 37, 'PHE'), ('A', 38, 'ILE'), ('A', 39, 'ILE'), ('A', 40, 'THR'), ('A', 41, 'ASN'), ('A', 42, 'LYS'), ('A', 43, 'HIS'), ('A', 44, 'LEU'), ('A', 45, 'PHE'), ('A', 46, 'ARG'), ('A', 47, 'ARG'), ('A', 48, 'ASN'), ('A', 49, 'ASN'), ('A', 50, 'GLY'), ('A', 51, 'THR'), ('A', 52, 'LEU'), ('A', 53, 'LEU'), ('A', 54, 'VAL'), ('A', 55, 'GLN'), ('A', 56, 'SER'), ('A', 57, 'LEU'), ('A', 58, 'HIS'), ('A', 59, 'GLY'), ('A', 60, 'VAL'), ('A', 61, 'PHE'), ('A', 62, 'LYS'), ('A', 63, 'VAL'), ('A', 64, 'LYS'), ('A', 65, 'ASN'), ('A', 66, 'THR'), ('A', 67, 'THR'), ('A', 68, 'THR'), ('A', 69, 'LEU'), ('A', 70, 'GLN'), ('A', 71, 'GLN'), ('A', 72, 'HIS'), ('A', 73, 'LEU'), ('A', 74, 'ILE'), ('A', 75, 'ASP'), ('A', 76, 'GLY'), ('A', 77, 'ARG'), ('A', 78, 'ASP'), ('A', 79, 'MET'), ('A', 80, 'ILE'), ('A', 81, 'ILE'), ('A', 82, 'ILE'), ('A', 83, 'ARG'), ('A', 84, 'MET'), ('A', 85, 'PRO'), ('A', 86, 'LYS'), ('A', 87, 'ASP'), ('A', 88, 'PHE'), ('A', 89, 'PRO'), ('A', 90, 'PRO'), ('A', 91, 'PHE'), ('A', 92, 'PRO'), ('A', 93, 'GLN'), ('A', 94, 'LYS'), ('A', 95, 'LEU'), ('A', 96, 'LYS'), ('A', 97, 'PHE'), ('A', 98, 'ARG'), ('A', 99, 'GLU'), ('A', 100, 'PRO'), ('A', 101, 'GLN'), ('A', 102, 'ARG'), ('A', 103, 'GLU'), ('A', 104, 'GLU'), ('A', 105, 'ARG'), ('A', 106, 'ILE'), ('A', 107, 'CYS'), ('A', 108, 'LEU'), ('A', 109, 'VAL'), ('A', 110, 'THR'), ('A', 111, 'THR'), ('A', 112, 'ASN'), ('A', 113, 'PHE'), ('A', 114, 'GLN'), ('A', 115, 'THR'), ('A', 116, 'LYS'), ('A', 117, 'SER'), ('A', 118, 'MET'), ('A', 119, 'SER'), ('A', 120, 'SER'), ('A', 121, 'MET'), ('A', 122, 'VAL'), ('A', 123, 'SER'), ('A', 124, 'ASP'), ('A', 125, 'THR'), ('A', 126, 'SER'), ('A', 127, 'CYS'), ('A', 128, 'THR'), ('A', 129, 'PHE'), ('A', 130, 'PRO'), ('A', 131, 'SER'), ('A', 132, 'SER'), ('A', 133, 'ASP'), ('A', 134, 'GLY'), ('A', 135, 'ILE'), ('A', 136, 'PHE'), ('A', 137, 'TRP'), ('A', 138, 'LYS'), ('A', 139, 'HIS'), ('A', 140, 'TRP'), ('A', 141, 'ILE'), ('A', 142, 'GLN'), ('A', 143, 'THR'), ('A', 144, 'LYS'), ('A', 145, 'ASP'), ('A', 146, 'GLY'), ('A', 147, 'GLN'), ('A', 148, 'CYS'), ('A', 149, 'GLY'), ('A', 150, 'SER'), ('A', 151, 'PRO'), ('A', 152, 'LEU'), ('A', 153, 'VAL'), ('A', 154, 'SER'), ('A', 155, 'THR'), ('A', 156, 'ARG'), ('A', 157, 'ASP'), ('A', 158, 'GLY'), ('A', 159, 'PHE'), ('A', 160, 'ILE'), ('A', 161, 'VAL'), ('A', 162, 'GLY'), ('A', 163, 'ILE'), ('A', 164, 'HIS'), ('A', 165, 'SER'), ('A', 166, 'ALA'), ('A', 167, 'SER'), ('A', 168, 'ASN'), ('A', 169, 'PHE'), ('A', 170, 'THR'), ('A', 171, 'ASN'), ('A', 172, 'THR'), ('A', 173, 'ASN'), ('A', 174, 'ASN'), ('A', 175, 'TYR'), ('A', 176, 'PHE'), ('A', 177, 'THR'), ('A', 178, 'SER'), ('A', 179, 'VAL'), ('A', 180, 'PRO'), ('A', 181, 'LYS'), ('A', 182, 'ASN'), ('A', 183, 'PHE'), ('A', 184, 'MET'), ('A', 185, 'GLU'), ('A', 186, 'LEU'), ('A', 187, 'LEU'), ('A', 188, 'THR'), ('A', 189, 'ASN'), ('A', 190, 'GLN'), ('A', 191, 'GLU'), ('A', 192, 'ALA'), ('A', 193, 'GLN'), ('A', 194, 'GLN'), ('A', 195, 'TRP'), ('A', 196, 'VAL'), ('A', 197, 'SER'), ('A', 198, 'GLY'), ('A', 199, 'TRP'), ('A', 200, 'ARG'), ('A', 201, 'LEU'), ('A', 202, 'ASN'), ('A', 203, 'ALA'), ('A', 204, 'ASP'), ('A', 205, 'SER'), ('A', 206, 'VAL'), ('A', 207, 'LEU'), ('A', 208, 'TRP'), ('A', 209, 'GLY'), ('A', 210, 'GLY'), ('A', 211, 'HIS'), ('A', 212, 'LYS'), ('A', 213, 'VAL'), ('A', 214, 'PHE'), ('A', 215, 'MET'), ('A', 216, 'VAL'), ('A', 217, 'LYS'), ('A', 218, 'PRO'), ('A', 219, 'GLU'), ('A', 220, 'GLU'), ('A', 221, 'PRO'), ('A', 222, 'PHE'), ('A', 223, 'GLN'), ('A', 224, 'PRO'), ('A', 225, 'VAL'), ('A', 226, 'LYS'), ('A', 227, 'GLU'), ('A', 228, 'ALA'), ('A', 229, 'THR'), ('A', 230, 'GLN'), ('A', 231, 'LEU'), ('A', 232, 'MET'), ('A', 233, 'ASN')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lmdb_dout' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m da\u001b[38;5;241m.\u001b[39mload_dataset(pdb_din, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m                             transform \u001b[38;5;241m=\u001b[39m myResTransform(balance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, pos_oi \u001b[38;5;241m=\u001b[39mpos_oi_all))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create LMDB dataset from PDB dataset, and write to file\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m da\u001b[38;5;241m.\u001b[39mmake_lmdb_dataset(dataset, \u001b[43mlmdb_dout\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lmdb_dout' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = da.load_dataset(pdb_din, 'pdb',\n",
    "                            transform = myResTransform(balance=False, pos_oi =pos_oi_all))\n",
    "\n",
    "# Create LMDB dataset from PDB dataset, and write to file\n",
    "da.make_lmdb_dataset(dataset, lmdb_dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atom3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
