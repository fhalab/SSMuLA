{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/disk2/fli/SSMuLA\n"
     ]
    }
   ],
   "source": [
    "%cd ~/SSMuLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/disk2/fli/SSMuLA/sandbox/esmif/score_log_likelihoods.py\", line 12, in <module>\n",
      "    from biotite.sequence.io.fasta import FastaFile, get_sequences\n",
      "ModuleNotFoundError: No module named 'biotite'\n"
     ]
    }
   ],
   "source": [
    "!python sandbox/esmif/score_log_likelihoods.py sandbox/esmif/5YH2.pdb \\\n",
    "  sandbox/esmif/5YH2_mutated_seqs.fasta --chain C \\\n",
    "  --outpath sandbox/esmif/5YH2_mutated_seqs_scores.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm\n",
    "# import esm.inverse_folding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm_if1_gvp4_t16_142M_UR50.pt\" to /disk2/fli/.cache/torch/hub/checkpoints/esm_if1_gvp4_t16_142M_UR50.pt\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, alphabet \u001b[38;5;241m=\u001b[39m \u001b[43mesm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm_if1_gvp4_t16_142M_UR50\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/SSMuLA/lib/python3.10/site-packages/esm/pretrained.py:347\u001b[0m, in \u001b[0;36mesm_if1_gvp4_t16_142M_UR50\u001b[0;34m()\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mesm_if1_gvp4_t16_142M_UR50\u001b[39m():\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Inverse folding model with 142M params, with 4 GVP-GNN layers, 8\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    Transformer encoder layers, and 8 Transformer decoder layers, trained on\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    CATH structures and 12 million alphafold2 predicted structures from UniRef50\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    Returns a tuple of (Model, Alphabet).\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_and_alphabet_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mesm_if1_gvp4_t16_142M_UR50\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/SSMuLA/lib/python3.10/site-packages/esm/pretrained.py:64\u001b[0m, in \u001b[0;36mload_model_and_alphabet_hub\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_and_alphabet_hub\u001b[39m(model_name):\n\u001b[1;32m     63\u001b[0m     model_data, regression_data \u001b[38;5;241m=\u001b[39m _download_model_and_regression_data(model_name)\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model_and_alphabet_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregression_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/SSMuLA/lib/python3.10/site-packages/esm/pretrained.py:193\u001b[0m, in \u001b[0;36mload_model_and_alphabet_core\u001b[0;34m(model_name, model_data, regression_data)\u001b[0m\n\u001b[1;32m    191\u001b[0m     model, alphabet, model_state \u001b[38;5;241m=\u001b[39m _load_model_and_alphabet_core_v2(model_data)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     model, alphabet, model_state \u001b[38;5;241m=\u001b[39m \u001b[43m_load_model_and_alphabet_core_v1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m expected_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(model\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    196\u001b[0m found_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(model_state\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[0;32m~/miniconda3/envs/SSMuLA/lib/python3.10/site-packages/esm/pretrained.py:129\u001b[0m, in \u001b[0;36m_load_model_and_alphabet_core_v1\u001b[0;34m(model_data)\u001b[0m\n\u001b[1;32m    126\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m esm\u001b[38;5;241m.\u001b[39mMSATransformer\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvariant_gvp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39march:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minverse_folding\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     model_type \u001b[38;5;241m=\u001b[39m esm\u001b[38;5;241m.\u001b[39minverse_folding\u001b[38;5;241m.\u001b[39mgvp_transformer\u001b[38;5;241m.\u001b[39mGVPTransformerModel\n\u001b[1;32m    132\u001b[0m     model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mvars\u001b[39m(model_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m])  \u001b[38;5;66;03m# convert Namespace -> dict\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/SSMuLA/lib/python3.10/site-packages/esm/inverse_folding/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gvp_transformer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multichain_util\n",
      "File \u001b[0;32m~/miniconda3/envs/SSMuLA/lib/python3.10/site-packages/esm/inverse_folding/gvp_transformer.py:16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transform\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mesm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Alphabet\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DihedralFeatures\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgvp_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GVPEncoder\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgvp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unflatten_graph\n",
      "File \u001b[0;32m~/miniconda3/envs/SSMuLA/lib/python3.10/site-packages/esm/inverse_folding/features.py:73\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgvp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flatten_graph\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgvp_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GVP, LayerNorm\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize, norm, nan_to_num, rbf\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGVPInputFeaturizer\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m~/miniconda3/envs/SSMuLA/lib/python3.10/site-packages/esm/inverse_folding/gvp_modules.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MessagePassing\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_scatter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scatter_add, scatter\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtuple_size\u001b[39m(tp):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m([\u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m a \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m a\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m tp])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "model, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_singlechain_backbone(model, alphabet, args):\n",
    "    if torch.cuda.is_available() and not args.nogpu:\n",
    "        model = model.cuda()\n",
    "        print(\"Transferred model to GPU\")\n",
    "    coords, native_seq = esm.inverse_folding.util.load_coords(args.pdbfile, args.chain)\n",
    "    print('Native sequence loaded from structure file:')\n",
    "    print(native_seq)\n",
    "    print('\\n')\n",
    "\n",
    "    ll, _ = esm.inverse_folding.util.score_sequence(\n",
    "            model, alphabet, coords, native_seq) \n",
    "    print('Native sequence')\n",
    "    print(f'Log likelihood: {ll:.2f}')\n",
    "    print(f'Perplexity: {np.exp(-ll):.2f}')\n",
    "\n",
    "    print('\\nScoring variant sequences from sequence file..\\n')\n",
    "\n",
    "    seqs = \n",
    "    Path(args.outpath).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(args.outpath, 'w') as fout:\n",
    "        fout.write('seqid,log_likelihood\\n')\n",
    "        for header, seq in tqdm(seqs.items()):\n",
    "            ll, _ = esm.inverse_folding.util.score_sequence(\n",
    "                    model, alphabet, coords, str(seq))\n",
    "            fout.write(header + ',' + str(ll) + '\\n')\n",
    "    print(f'Results saved to {args.outpath}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSMuLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
